{"title":"コードで理解するTransformer：AttentionとGPTモデル入門","markdown":{"yaml":{"title":"コードで理解するTransformer：AttentionとGPTモデル入門","author":"Junichiro Iwasawa","date":"2025-04-11","categories":["Machine Learning","Transformer","Python"],"image":"https://picsum.photos/id/83/200"},"headingText":"準備：言語モデリングの基本","containsRefs":false,"markdown":"\n\n近年、ChatGPTやGPT-4といった大規模言語モデル（LLM: Large Language Models）が大きな注目を集めています。これらのモデルは、コードの作成、メールの下書き、複雑な質問への回答、さらには創造的な文章生成まで、驚くべき能力を発揮します。これらのシステムの多くを支える中核技術が、2017年の画期的な論文「[Attention is All You Need](https://arxiv.org/abs/1706.03762)」で提案されたTransformerアーキテクチャです。\n\nしかし、この「Attention」メカニズムとは一体何で、どのようにしてGPTのようなモデルが文脈を理解し、一貫性のあるテキストを生成することを可能にしているのでしょうか？\n\nAndrej Karpathy氏の優れた動画「[Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY)」では、彼が[`nanogpt`](https://github.com/karpathy/ng-video-lecture/tree/master)と呼ぶ小規模なバージョンをゼロから構築することで、Transformerを分かりやすく解説しています。今回は、彼の解説に沿って、Transformerの心臓部であるself-attentionの仕組みを解き明かしていきましょう。\n\nAttentionに入る前に、基本的なタスクである「言語モデリング」について理解しましょう。言語モデリングの目標は、与えられたシーケンス（文脈）に基づいて、シーケンス中の次の単語（または文字、トークン）を予測することです。\n\nKarpathy氏はまず、「Tiny Shakespeare」データセットを使用します。これはシェイクスピアの作品を連結した単一のテキストファイルです。\n\n```python\n# まずは学習用のデータセットを用意します。Tiny Shakespeareデータセットをダウンロードしましょう。\n!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n\n# 中身を確認するために読み込みます。\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# このテキストに含まれるユニークな文字をすべてリストアップします。\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\n# !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\nprint(vocab_size)\n# 65\n\n# 文字から整数へのマッピングを作成します。\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: 文字列を受け取り、整数のリストを出力\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: 整数のリストを受け取り、文字列を出力\nprint(encode(\"hii there\"))\n# [46, 47, 47, 1, 58, 46, 43, 56, 43]\nprint(decode(encode(\"hii there\")))\n# hii there\n```\n```python\n# テキストデータセット全体をエンコードし、torch.Tensorに格納します。\nimport torch # PyTorchを使用します: https://pytorch.org\ndata = torch.tensor(encode(text), dtype=torch.long)\nprint(data.shape, data.dtype)\n# torch.Size([1115394]) torch.int64\nprint(data[:1000])\n# tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44, ...\n```\n\nこの例では、テキストは文字レベルでトークン化（tokenized）され、各文字が数にマッピングされます。モデルの役割は、数のシーケンスが与えられたときに、次に来る文字の数を予測することです。\n\nKarpathy氏は、まず最も単純な言語モデルである**Bigram Model**を実装します。\n\n```python\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # 各トークンはルックアップテーブルから次のトークンのロジットを直接読み取る\n        # 動画では後に vocab_size x n_embd に変更される\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        # idx と targets は両方とも (B,T) の整数テンソル\n        # Bigramモデルではロジットは直接ルックアップされる\n        logits = self.token_embedding_table(idx) # (B,T,C) ここで初期はC=vocab_size\n\n        if targets is None:\n            loss = None\n        else:\n            # cross_entropyのために形状を変更\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idxは現在の文脈におけるインデックスの(B, T)配列\n        for _ in range(max_new_tokens):\n            # 予測を取得\n            logits, loss = self(idx)\n            # 最後のタイムステップのみに注目\n            logits = logits[:, -1, :] # (B, C) になる\n            # softmaxを適用して確率を取得\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # 分布からサンプリング\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # サンプリングされたインデックスを実行中のシーケンスに追加\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)  # torch.Size([32, 65])\nprint(loss)  # tensor(4.8786, grad_fn=<NllLossBackward0>)\n\nprint(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n# SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGpwnYWmnxKWWev-tDqXErVKLgJ\n```\nこのモデルを実際に訓練してみます。\n```python\n# PyTorch optimizerの作成\noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n\nbatch_size = 32\nfor steps in range(100): # increase number of steps for good results...\n\n    # batch の作成\n    xb, yb = get_batch('train')\n\n    # lossをもとに重みを更新\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nprint(loss.item())  # 4.65630578994751\nprint(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))\n# oTo.JUZ!!zqe!\n# xBP qbs$Gy'AcOmrLwwt ...\n```\nこのモデルは、入力文字のインデックスを使って、次の文字の確率分布（ロジット）を直接ルックアップする埋め込み（embedding）テーブルを使用します。これは単純ですが、重大な欠点があります。それは、文脈を完全に無視してしまう点です。「hat」の後の「t」も、「bat」の後の「t」も、予測は同じになってしまいます。トークン同士が「対話」していないのです。\n\n## コミュニケーションの必要性：過去の情報を集約する\nより良い予測を行うためには、トークンはシーケンス内の先行するトークンからの情報を必要とします。トークンはどのようにしてコミュニケーションできるのでしょうか？\n\nKarpathy氏は、行列積を用いた「数学的なトリック」を紹介します。トークンが文脈を得る最も簡単な方法は、自身を含む先行するすべてのトークンからの情報を平均化することです。\n\n入力`x`が`(B, T, C)`（Batch、Time（シーケンス長）、Channels（埋め込み次元））の形状を持つとします。`xbow[b, t]`が`x[b, 0]`から`x[b, t]`までの平均を含むような`xbow`（bag-of-words表現）を計算したいと考えます。\n\n以下のような単純なループは非効率です。\n```python\n# xbow[b,t] = mean_{i<=t} x[b,i] を計算したい\n# (xがB, T, Cの形状で定義されていると仮定)\nB,T,C = 4,8,32 # 例としての次元\nx = torch.randn(B,T,C)\nxbow = torch.zeros((B,T,C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,:t+1] # (t+1, C)\n        xbow[b,t] = torch.mean(xprev, 0)\n```\n効率的な方法は、下三角行列との行列積を使用することです。\n```python\n# version 2: 行列積を用いた重み付き集約\nT = 8 # 例としてのシーケンス長\nwei = torch.tril(torch.ones(T, T)) # 1で構成される下三角行列\nwei = wei / wei.sum(1, keepdim=True) # 各行の合計が1になるように正規化 -> 平均化\n# 例として B=4, T=8, C=32 のx\nx = torch.randn(4, T, 32)\nxbow2 = wei @ x # (T, T) @ (B, T, C) はブロードキャストされ -> (B, T, C)\ntorch.allclose(xbow, xbow2)  # True\n```\nここで、`wei`（重み）は`(T, T)`行列です。`wei`の行`t`は、列0からtまでのみ非ゼロ値（この場合は1/(t+1)）を持ちます。これを`x`（形状`(B, T, C)`）と乗算すると、PyTorchは`wei`をバッチ次元全体にブロードキャストします。結果として得られる`xbow2[b, t]`は、`x[b, 0]`から`x[b, t]`までの重み付き合計（この場合は平均）となります。\n\nこの行列積は効率的に集約処理を実行します。これは`softmax`を使っても実現できます。\n```python\n# version 3: Softmaxを使用\nT = 8\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf')) # 上三角部分を-infで埋める\nwei = F.softmax(wei, dim=-1) # Softmaxは行の合計を1にし、平均の重みを回復する\nxbow3 = wei @ x\n# torch.allclose(xbow, xbow3) は True になるはず\n```\nなぜここで`softmax`を使うかというと、重み（`wei`）が固定された平均である必要はなく、重み自体が学習可能であったり、データに依存したりできるという重要なアイデアを導入するからです。これこそが、self-attentionが行うことです。\n\n## 位置情報の導入：Position Encoding\nSelf-Attentionメカニズム自体について詳しく見る前に、もう一つ重要な要素について触れておく必要があります。それは、トークンの位置に関する情報です。\n\nSelf-Attentionの基本的な計算（Query, Key, Valueを用いた加重集約）は、それ自体ではトークンがシーケンス内のどの位置にあるかを考慮しません。極端な話、単語の順番が入れ替わっても、各トークン間のAttentionスコアの計算自体は（入力ベクトルが同じであれば）変わりません。これでは、文の意味を正しく捉えることができません。「猫がマットの上に座った」と「マットが猫の上に座った」では意味が全く異なります。\n\nこの問題を解決するため、Transformerではトークン自体の意味を表す埋め込みベクトル（Token Embedding）に、そのトークンがシーケンス中のどの位置にあるかを示すPosition Encoding（位置エンコーディング）ベクトルを加算します。\n\nKarpathy氏の動画で実装されている`nanogpt`では、学習可能なPosition Encodingが用いられています。具体的には、`block_size`（扱える最大のシーケンス長）に対応する数の位置ベクトルを格納する埋め込みテーブル（`position_embedding_table`）を用意します。シーケンス長が`T`の場合、`0`から`T-1`までの整数をインデックスとして、対応する位置ベクトルをこのテーブルから取得します。\n```python\n# BigramLanguageModel内のforwardメソッドより抜粋\nB, T = idx.shape\n\n# idx and targets are both (B,T) tensor of integers\ntok_emb = self.token_embedding_table(idx) # (B,T,C) - トークン埋め込み\n# torch.arange(T, device=device) は 0 から T-1 までの整数のシーケンスを生成\npos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C) - 位置埋め込み\nx = tok_emb + pos_emb # (B,T,C) - トークン埋め込みと位置埋め込みを加算\nx = self.blocks(x) # ... このxがTransformerブロックへの入力となる ...\n```\nこのようにして、トークン自体の情報(`tok_emb`)とその位置情報(`pos_emb`)の両方を含んだベクトル`x`が作成されます。この`x`こそが、後続のTransformerブロック（Self-Attention層やFeedForward層）への実際の入力となるのです。これにより、モデルはトークンの意味だけでなく、その順序関係も考慮して処理を進めることができるようになります。\n\n\n## Self-Attention：データに基づいた情報の集約\n単純な平均化は、過去のすべてのトークンを平等に扱います。しかし、実際には、過去の一部のトークンが他のトークンよりもはるかに重要である場合があります。例えば、「The cat sat on the...」の次に続く単語を予測する場合、「The」よりも「cat」という単語の方が重要である可能性が高いです。\n\nSelf-attentionは、トークンが他のトークンに**問い合わせ（query）を行い、関連性に基づいて注意スコア（attention scores）**を割り当てることを可能にします。各トークンは3つのベクトルを生成します。\n\n1. Query (Q): 自分はどのような情報を探しているか？\n2. Key (K): 自分はどのような情報を持っているか？\n3. Value (V): もし自分に注意が向けられたら、どのような情報を提供するか？\n\nトークン`i`とトークン`j`間の注意スコア（またはaffinity）は、トークン`i`のQueryベクトル(`q_i`)とトークン`j`のKeyベクトル(`k_j`)の内積を取ることで計算されます。\n\n`affinity(i, j) = q_i ⋅ k_j`\n\n内積が大きい場合、QueryがKeyに良く一致していることを意味し、トークン`j`がトークン`i`にとって関連性が高いと判断されます。\n\n以下は、Attentionの単一の「Head」を実装する方法です。\n\n```python\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB,T,C = 4,8,32 # batch, time, channels (埋め込み次元)\nx = torch.randn(B,T,C) # 入力トークンの埋め込み + 位置エンコーディング\n\n# 単一のHeadがself-attentionを実行する様子を見てみましょう\nhead_size = 16 # このHeadのK, Q, Vベクトルの次元\n# 入力'x'をK, Q, Vに射影するための線形層\nkey   = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\n\nk = key(x)   # (B, T, head_size)\nq = query(x) # (B, T, head_size)\n\n# 注意スコア（\"affinities\"）を計算\n# (B, T, head_size) @ (B, head_size, T) ---> (B, T, T)\nwei =  q @ k.transpose(-2, -1)\n\n# --- スケーリングステップ (後述) ---\nwei = wei * (head_size**-0.5) # アフィニティをスケーリング\n\n# --- Decoderのためのマスキング ---\ntril = torch.tril(torch.ones(T, T, device=x.device)) # xと同じデバイスを使用\nwei = wei.masked_fill(tril == 0, float('-inf')) # 未来のトークンをマスク\n\n# --- スコアを正規化して確率を取得 ---\nwei = F.softmax(wei, dim=-1) # (B, T, T)\n\n# --- Valueの重み付き集約を実行 ---\nv = value(x) # (B, T, head_size)\n# (B, T, T) @ (B, T, head_size) ---> (B, T, head_size)\nout = wei @ v\n\n# out.shape は (B, T, head_size)\n```\n\n重要なステップを分解してみましょう。\n\n1. 射影（Projection）: 入力`x`（トークン埋め込みと位置エンコーディングを含む）が、線形層によってK、Q、V空間に射影されます。\n2. アフィニティ計算（Affinity Calculation）: `q @ k.transpose(...)` は、バッチ内の各シーケンスにおける全てのQueryベクトルとKeyベクトルのペアの内積を計算します。これにより、生の注意スコアである`wei`（形状 `B, T, T`）が得られます。\n3. スケーリング（Scaling）: スコア`wei`は`head_size`の平方根でスケールダウンされます。これは、特に初期化段階での学習を安定させるために重要です。スケーリングがないと、内積の分散がhead_sizeと共に増加し、`softmax`の入力が勾配の非常に小さい領域に押しやられ、学習が妨げられる可能性があります。\n4. マスキング（Masking (Decoder固有)）: GPTのような自己回帰型（autoregressive）言語モデリングでは、位置`t`のトークンは位置`t`までのトークンにのみ注意を向けるべきです。これは、未来の位置（`j > t`）に対応する注意スコアを下三角行列（`tril`）を用いた`masked_fill`で負の無限大に設定することで実現されます。これにより、`softmax`は未来のトークンにゼロの確率を割り当てます。（BERTのようなEncoderブロックでは、この causal mask は使用されません。）\n5. Softmax: マスクされたスコアに対して行ごとに`softmax`を適用します。これにより、スコアは各トークン`t`について合計が1になる確率に変換され、先行するトークン`0`から`t`までの注意分布を表します。\n6. Valueの集約（Value Aggregation）: 各トークン`t`の最終出力`out`は、`wei`内の注意確率によって重み付けされた、全トークンのValueベクトル（`v`）の重み付き合計です。`out = wei @ v`。\n\n出力`out`（形状 `B, T, head_size`）は、学習されたK、Q、Vの射影に基づいて、シーケンス内の他の関連トークンから集約された情報を各トークンごとに含んでいます。\n\n## Multi-Head Attention：多角的な視点\n単一のAttention Headは、ある特定タイプの関係性（例：名詞と動詞の一致）に焦点を当てるかもしれません。多様な関係性を捉えるために、Transformerは**Multi-Head Attention**を使用します。\n\n```python\nclass Head(nn.Module):\n    \"\"\" self-attentionの単一ヘッド \"\"\"\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        # trilをバッファとして登録（パラメータではない）\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n        self.dropout = nn.Dropout(dropout) # Dropoutを追加\n\n    def forward(self, x):\n        B,T,C = x.shape\n        k = self.key(x)   # (B,T,head_size)\n        q = self.query(x) # (B,T,head_size)\n        # 注意スコア（\"affinities\"）を計算\n        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # head_sizeでスケーリング\n        # Tに基づいて動的にマスクを適用\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n        wei = F.softmax(wei, dim=-1)\n        wei = self.dropout(wei) # 注意の重みにDropoutを適用\n        # Valueの重み付き集約を実行\n        v = self.value(x) # (B,T,head_size)\n        out = wei @ v\n        return out\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\" self-attentionの複数ヘッドを並列に実行 \"\"\"\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        # 複数のHeadインスタンスを作成\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        # 連結後の射影層\n        self.proj = nn.Linear(num_heads * head_size, n_embd) # n_embd = num_heads * head_size\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        # 各ヘッドを並列に実行し、結果をチャネル次元で連結\n        out = torch.cat([h(x) for h in self.heads], dim=-1) # (B, T, num_heads * head_size)\n        # 連結された出力を元のn_embd次元に再射影\n        out = self.dropout(self.proj(out)) # (B, T, n_embd)\n        return out\n```\nこれは単純に複数のHeadモジュールを並列に実行し、それぞれが異なる学習済みK、Q、V射影を持つ可能性があります。各ヘッドの出力（それぞれ `B, T, head_size`）は連結され（`B, T, num_heads * head_size`）、その後、別の線形層（`self.proj`）を用いて元の埋め込み次元（`B, T, n_embd`）に再射影されます。これにより、モデルは異なる表現部分空間からの情報に同時に注意を向けることができます。\n\n## Attentionの応用：Self-Attention, Cross-Attention, Encoder/Decoderブロック\nこれまで解説してきたAttentionの基本的な仕組みは、**Self-Attention**と呼ばれるものでした。これはQuery(Q), Key(K), Value(V)のベクトルがすべて同じ入力シーケンス（`x`）から生成され、シーケンス内のトークンが相互に注意を向け合うものでした。しかし、このSelf-Attentionの使われ方や、Attentionメカニズム全体にはいくつかの重要なバリエーションが存在します。\n\nまず、Self-Attention自体の使われ方によって、それがEncoderブロックの一部として機能するのか、Decoderブロックの一部として機能するのかが変わってきます。この違いを生む主な要因は、Attentionスコア計算におけるマスキングの有無です。\n\nDecoderブロックで使われるSelf-Attentionでは、未来の情報を参照しないようにするための因果マスキング（causal masking）、つまり三角マスクが適用されます。これは、GPTのような自己回帰（autoregressive）モデルや、機械翻訳のデコーダー部分のように、過去の情報のみに基づいて次のトークンを生成する必要があるタスクで不可欠です。Karpathy氏の動画で構築された`nanogpt`は、まさしくこのDecoderブロックのみで構成されるモデルです。\n\n一方、Encoderブロックで使われるSelf-Attentionでは、この因果マスキングは適用されません。シーケンス内のすべてのトークンが、他のすべてのトークン（過去も未来も含む）に自由に注意を向けることができます。これは、BERTのように入力テキスト全体の文脈理解を目的とするモデルや、機械翻訳におけるエンコーダー部分（入力文全体の情報を符号化する役割）などで用いられます。入力シーケンス全体の双方向の文脈を捉えるのに適しています。\n\n次に、Attentionメカニズムのもう一つの重要な形態が**Cross-Attention**です。これはSelf-Attention（マスキングの有無に関わらず）とは異なり、Query、Key、Valueの由来が異なります。Cross-Attentionでは、Query(Q)はあるソース（例えばデコーダー側の状態）から生成されますが、Key(K)とValue(V)は別のソース（例えばエンコーダーの最終出力）から提供されます。\n\nこのCross-Attentionは、主にEncoder-Decoderアーキテクチャにおいて、EncoderとDecoderを接続する役割を果たします。デコーダーが出力トークンを生成する際に、Cross-Attentionを通じてエンコーダーが符号化した入力情報全体を常に参照できるようにします。機械翻訳タスクで、翻訳先の言語を生成しながら常に翻訳元の文章の意味を考慮する、といったことを可能にするメカニズムです。\n\n`nanogpt`のようなdecoder-onlyモデルでは、外部の入力シーケンスを処理するEncoder部分が存在しないため、EncoderブロックやCross-Attentionは必要なく、因果マスキングを用いたSelf-Attention（Decoderブロック）のみで構成されている、というわけです。\n\n## Transformerブロック：通信と計算\nAttentionは通信メカニズムを提供します。しかし、モデルは集約された情報を処理するための計算も必要です。標準的なTransformerブロックは、Multi-Head Self-Attentionと、単純な位置ごとのFeedForwardネットワークを組み合わせます。\n\n重要な点として、各サブレイヤー（AttentionとFeedForward）の周囲に**Residual Connections（残差接続）とLayer Normalization（層正規化）**が追加されます。\n\n- **Residual Connections**: `x = x + sublayer(norm(x))`。サブレイヤーの入力`x`が、サブレイヤーの出力に加算されます。これにより、深いネットワークでの逆伝播時に勾配が流れやすくなり、学習の安定性と性能が大幅に向上します。\n- **Layer Normalization**: 各トークンについて、特徴量をチャネル次元にわたって独立に正規化します。Batch Normalizationとは異なり、バッチ統計に依存しないため、シーケンスデータに適しています。これも学習を安定させます。Karpathy氏は、サブレイヤーの前にLayerNormを適用する一般的な「pre-norm」形式を実装しています。\n\n```python\nclass FeedFoward(nn.Module):\n    \"\"\" 単純な線形層と非線形活性化関数 \"\"\"\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd), # 中間層は通常4倍大きい\n            nn.ReLU(),                    # ReLU活性化関数\n            nn.Linear(4 * n_embd, n_embd), # n_embdに再射影\n            nn.Dropout(dropout),           # 正則化のためのDropout\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformerブロック：通信の後に計算 \"\"\"\n    def __init__(self, n_embd, n_head):\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size) # 通信 (Communication)\n        self.ffwd = FeedFoward(n_embd)                 # 計算 (Computation)\n        self.ln1 = nn.LayerNorm(n_embd)                # Attention前のLayerNorm\n        self.ln2 = nn.LayerNorm(n_embd)                # FeedForward前のLayerNorm\n\n    def forward(self, x):\n        # Pre-norm形式と残差接続\n        # LayerNorm適用 -> Self-Attention -> 残差を加算\n        x = x + self.sa(self.ln1(x))\n        # LayerNorm適用 -> FeedForward -> 残差を加算\n        x = x + self.ffwd(self.ln2(x))\n        return x\n```\n完全なGPTモデルは、これらのBlockレイヤーを複数、順番に積み重ねます。すべてのブロックを通過した後、最終的なLayerNormが適用され、その後、最終的なトークン表現を語彙サイズに射影する線形層が続き、次のトークンを予測するためのロジットが得られます。\n\n## 最終的なGPTモデルの構築\nこれまで解説してきたコンポーネントを統合し、最終的なGPTスタイルの言語モデル`GPTLanguageModel`を構築します。以下に示すコードは、Karpathy氏の動画における完成形であり、先に説明した`Block`（`MultiHeadAttention`と`FeedForward`を含む）などを組み合わせています。\n\n```python\n# (主要なハイパーパラメータを再掲)\n# hyperparameters\nbatch_size = 64 # 並列処理する独立したシーケンス数\nblock_size = 256 # 予測のための最大コンテキスト長\nmax_iters = 5000\neval_interval = 500\nlearning_rate = 3e-4\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 384     # 埋め込み次元数\nn_head = 6       # Attentionヘッドの数\nn_layer = 6      # Transformerブロックの層数\ndropout = 0.2    # ドロップアウト率\n# ------------\n\nclass GPTLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # トークン埋め込みと位置埋め込みのテーブル\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        # n_layer個のTransformerブロックを積み重ねる\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd) # 最終LayerNorm\n        self.lm_head = nn.Linear(n_embd, vocab_size) # 出力層（線形層）\n\n        # （動画本編では触れられていないが重要な）重み初期化\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        # （重み初期化の詳細は省略）\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n\n        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n        x = tok_emb + pos_emb # (B,T,C)\n        x = self.blocks(x) # (B,T,C) Transformerブロックを通過\n        x = self.ln_f(x) # (B,T,C) 最終LayerNormを適用\n        logits = self.lm_head(x) # (B,T,vocab_size) LMヘッドでロジットを計算\n\n        if targets is None:\n            loss = None\n        else:\n            # 損失計算のために形状を変更\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idxは現在の文脈におけるインデックスの(B, T)配列\n        for _ in range(max_new_tokens):\n            # Position Embeddingのサイズ制限のため、idxを最後のblock_sizeトークンに切り詰める\n            idx_cond = idx[:, -block_size:]\n            # 予測を取得\n            logits, loss = self(idx_cond) # forwardパスを実行\n            # 最後のタイムステップのみに注目\n            logits = logits[:, -1, :] # (B, C) になる\n            # softmaxを適用して確率を取得\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # 分布からサンプリング\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # サンプリングされたインデックスを実行中のシーケンスに追加\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n```\n\nこのGPTLanguageModelクラスでは、`__init__`メソッドで、これまで説明してきたトークン埋め込みと位置埋め込みテーブル(`token_embedding_table`, `position_embedding_table`)を定義した後、`n_layer`個の`Block`を`nn.Sequential`で積み重ねています。これがTransformerの中核部であり、入力ベクトルはここを通過することで段階的にリッチな表現へと変換されます。その後、最終的な`LayerNorm` (`ln_f`)を経て、出力用の線形層`lm_head`によって語彙数次元のロジットへと変換されます。また、安定した学習のための重み初期化メソッド`_init_weights`も含まれています。\n\n`forward`メソッドは、この一連の流れを実装しており、トークン埋め込みと位置埋め込みを加算したベクトルを`blocks`に通し、正規化と線形変換を経て最終的なロジットを出力します。\n\nテキスト生成を行う`generate`メソッドでは、自己回帰的にトークンを生成していきますが、ここで重要なのは`idx_cond = idx[:, -block_size:]`の部分です。位置埋め込みテーブル`position_embedding_table`のサイズが`block_size`に固定されているため、モデルに入力できるのは直近`block_size`個のトークンまでとなります。この制約のもとでforwardパスを実行し、最後のタイムステップのロジットから次のトークンをサンプリングし、シーケンスを伸長していく処理を繰り返します。\n\nコード全体を見ると、これらのモデル定義に加えて、学習を制御するハイパーパラメータ群（`batch_size`や`learning_rate`など）や、`AdamW`オプティマイザ、そして`estimate_loss`関数を用いた評価を含む標準的な学習ループが組み合わされていることがわかります。これらが一体となってGPTモデルの学習と推論を実現しています。\n\n## スケールアップと結果\nKarpathy氏は上の`GPTLanguageModel`（`n_layer=6, n_head=6, n_embd=384, dropout=0.2`）でTiny Shakespeareを学習させます。結果として得られるモデルは、はるかに一貫性のある（ただし、まだ意味をなさない）シェイクスピア風のテキストを生成し、十分なモデル容量と組み合わされたAttentionの力を示しています。\n\n```console\n# GPTLanguageModelからのサンプル出力\nFlY BOLINGLO:\nThem thrumply towiter arts the\nmuscue rike begatt the sea it\nWhat satell in rowers that some than othis Marrity.\n\nLUCENTVO:\nBut userman these that, where can is not diesty rege;\nWhat and see to not. But's eyes. What?\n```\n\nこのアーキテクチャ、すなわち**decoder-only Transformer**（causal maskを使用）は、基本的にGPT-2やGPT-3のようなモデルで使用されているものと同じですが、パラメータ数、層数、埋め込みサイズ、そして学習データ（シェイクスピアだけでなく膨大なインターネットテキスト）の点で、はるかに大規模になっています。\n\n## まとめ\nAttentionメカニズム、特にScaled dot-product self-attentionは、Transformerの能力を飛躍的に向上させた革新的な技術です。これにより、シーケンス内のトークンが動的にお互いを参照し、学習されたQuery-Keyの相互作用に基づいて関連性スコア（アフィニティ）を計算し、関連するトークンのValueベクトルからの情報を重み付きで集約することが可能になります。Multi-Head Attention、Residual Connections、Layer Normalization、そして位置ごとのFeedForwardネットワークと組み合わせることで、ChatGPTのようなAIに革命をもたらしているモデルの基本的な構成要素であるTransformerブロックが形成されます。\n\nKarpathy氏のように段階的に構築することで、強力でありながらも、その中心的なアイデアは把握可能であり、比較的簡潔なコードで実装できることがわかります。\n\n---\n\n*この記事は、Andrej Karpathy氏のYouTube動画「[Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY)」に基づいています。完全なコードやより深い洞察については、ぜひ動画と彼の[`nanogpt`](https://github.com/karpathy/ng-video-lecture/tree/master)リポジトリをご覧ください。*\n*この記事が、TransformerとAttentionの理解の一助となれば幸いです。*\n","srcMarkdownNoYaml":"\n\n近年、ChatGPTやGPT-4といった大規模言語モデル（LLM: Large Language Models）が大きな注目を集めています。これらのモデルは、コードの作成、メールの下書き、複雑な質問への回答、さらには創造的な文章生成まで、驚くべき能力を発揮します。これらのシステムの多くを支える中核技術が、2017年の画期的な論文「[Attention is All You Need](https://arxiv.org/abs/1706.03762)」で提案されたTransformerアーキテクチャです。\n\nしかし、この「Attention」メカニズムとは一体何で、どのようにしてGPTのようなモデルが文脈を理解し、一貫性のあるテキストを生成することを可能にしているのでしょうか？\n\nAndrej Karpathy氏の優れた動画「[Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY)」では、彼が[`nanogpt`](https://github.com/karpathy/ng-video-lecture/tree/master)と呼ぶ小規模なバージョンをゼロから構築することで、Transformerを分かりやすく解説しています。今回は、彼の解説に沿って、Transformerの心臓部であるself-attentionの仕組みを解き明かしていきましょう。\n\n## 準備：言語モデリングの基本\nAttentionに入る前に、基本的なタスクである「言語モデリング」について理解しましょう。言語モデリングの目標は、与えられたシーケンス（文脈）に基づいて、シーケンス中の次の単語（または文字、トークン）を予測することです。\n\nKarpathy氏はまず、「Tiny Shakespeare」データセットを使用します。これはシェイクスピアの作品を連結した単一のテキストファイルです。\n\n```python\n# まずは学習用のデータセットを用意します。Tiny Shakespeareデータセットをダウンロードしましょう。\n!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n\n# 中身を確認するために読み込みます。\nwith open('input.txt', 'r', encoding='utf-8') as f:\n    text = f.read()\n\n# このテキストに含まれるユニークな文字をすべてリストアップします。\nchars = sorted(list(set(text)))\nvocab_size = len(chars)\nprint(''.join(chars))\n# !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\nprint(vocab_size)\n# 65\n\n# 文字から整数へのマッピングを作成します。\nstoi = { ch:i for i,ch in enumerate(chars) }\nitos = { i:ch for i,ch in enumerate(chars) }\nencode = lambda s: [stoi[c] for c in s] # encoder: 文字列を受け取り、整数のリストを出力\ndecode = lambda l: ''.join([itos[i] for i in l]) # decoder: 整数のリストを受け取り、文字列を出力\nprint(encode(\"hii there\"))\n# [46, 47, 47, 1, 58, 46, 43, 56, 43]\nprint(decode(encode(\"hii there\")))\n# hii there\n```\n```python\n# テキストデータセット全体をエンコードし、torch.Tensorに格納します。\nimport torch # PyTorchを使用します: https://pytorch.org\ndata = torch.tensor(encode(text), dtype=torch.long)\nprint(data.shape, data.dtype)\n# torch.Size([1115394]) torch.int64\nprint(data[:1000])\n# tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44, ...\n```\n\nこの例では、テキストは文字レベルでトークン化（tokenized）され、各文字が数にマッピングされます。モデルの役割は、数のシーケンスが与えられたときに、次に来る文字の数を予測することです。\n\nKarpathy氏は、まず最も単純な言語モデルである**Bigram Model**を実装します。\n\n```python\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\ntorch.manual_seed(1337)\n\nclass BigramLanguageModel(nn.Module):\n\n    def __init__(self, vocab_size):\n        super().__init__()\n        # 各トークンはルックアップテーブルから次のトークンのロジットを直接読み取る\n        # 動画では後に vocab_size x n_embd に変更される\n        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n\n    def forward(self, idx, targets=None):\n        # idx と targets は両方とも (B,T) の整数テンソル\n        # Bigramモデルではロジットは直接ルックアップされる\n        logits = self.token_embedding_table(idx) # (B,T,C) ここで初期はC=vocab_size\n\n        if targets is None:\n            loss = None\n        else:\n            # cross_entropyのために形状を変更\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idxは現在の文脈におけるインデックスの(B, T)配列\n        for _ in range(max_new_tokens):\n            # 予測を取得\n            logits, loss = self(idx)\n            # 最後のタイムステップのみに注目\n            logits = logits[:, -1, :] # (B, C) になる\n            # softmaxを適用して確率を取得\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # 分布からサンプリング\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # サンプリングされたインデックスを実行中のシーケンスに追加\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n\nm = BigramLanguageModel(vocab_size)\nlogits, loss = m(xb, yb)\nprint(logits.shape)  # torch.Size([32, 65])\nprint(loss)  # tensor(4.8786, grad_fn=<NllLossBackward0>)\n\nprint(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n# SKIcLT;AcELMoTbvZv C?nq-QE33:CJqkOKH-q;:la!oiywkHjgChzbQ?u!3bLIgwevmyFJGUGpwnYWmnxKWWev-tDqXErVKLgJ\n```\nこのモデルを実際に訓練してみます。\n```python\n# PyTorch optimizerの作成\noptimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n\nbatch_size = 32\nfor steps in range(100): # increase number of steps for good results...\n\n    # batch の作成\n    xb, yb = get_batch('train')\n\n    # lossをもとに重みを更新\n    logits, loss = m(xb, yb)\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n\nprint(loss.item())  # 4.65630578994751\nprint(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))\n# oTo.JUZ!!zqe!\n# xBP qbs$Gy'AcOmrLwwt ...\n```\nこのモデルは、入力文字のインデックスを使って、次の文字の確率分布（ロジット）を直接ルックアップする埋め込み（embedding）テーブルを使用します。これは単純ですが、重大な欠点があります。それは、文脈を完全に無視してしまう点です。「hat」の後の「t」も、「bat」の後の「t」も、予測は同じになってしまいます。トークン同士が「対話」していないのです。\n\n## コミュニケーションの必要性：過去の情報を集約する\nより良い予測を行うためには、トークンはシーケンス内の先行するトークンからの情報を必要とします。トークンはどのようにしてコミュニケーションできるのでしょうか？\n\nKarpathy氏は、行列積を用いた「数学的なトリック」を紹介します。トークンが文脈を得る最も簡単な方法は、自身を含む先行するすべてのトークンからの情報を平均化することです。\n\n入力`x`が`(B, T, C)`（Batch、Time（シーケンス長）、Channels（埋め込み次元））の形状を持つとします。`xbow[b, t]`が`x[b, 0]`から`x[b, t]`までの平均を含むような`xbow`（bag-of-words表現）を計算したいと考えます。\n\n以下のような単純なループは非効率です。\n```python\n# xbow[b,t] = mean_{i<=t} x[b,i] を計算したい\n# (xがB, T, Cの形状で定義されていると仮定)\nB,T,C = 4,8,32 # 例としての次元\nx = torch.randn(B,T,C)\nxbow = torch.zeros((B,T,C))\nfor b in range(B):\n    for t in range(T):\n        xprev = x[b,:t+1] # (t+1, C)\n        xbow[b,t] = torch.mean(xprev, 0)\n```\n効率的な方法は、下三角行列との行列積を使用することです。\n```python\n# version 2: 行列積を用いた重み付き集約\nT = 8 # 例としてのシーケンス長\nwei = torch.tril(torch.ones(T, T)) # 1で構成される下三角行列\nwei = wei / wei.sum(1, keepdim=True) # 各行の合計が1になるように正規化 -> 平均化\n# 例として B=4, T=8, C=32 のx\nx = torch.randn(4, T, 32)\nxbow2 = wei @ x # (T, T) @ (B, T, C) はブロードキャストされ -> (B, T, C)\ntorch.allclose(xbow, xbow2)  # True\n```\nここで、`wei`（重み）は`(T, T)`行列です。`wei`の行`t`は、列0からtまでのみ非ゼロ値（この場合は1/(t+1)）を持ちます。これを`x`（形状`(B, T, C)`）と乗算すると、PyTorchは`wei`をバッチ次元全体にブロードキャストします。結果として得られる`xbow2[b, t]`は、`x[b, 0]`から`x[b, t]`までの重み付き合計（この場合は平均）となります。\n\nこの行列積は効率的に集約処理を実行します。これは`softmax`を使っても実現できます。\n```python\n# version 3: Softmaxを使用\nT = 8\ntril = torch.tril(torch.ones(T, T))\nwei = torch.zeros((T,T))\nwei = wei.masked_fill(tril == 0, float('-inf')) # 上三角部分を-infで埋める\nwei = F.softmax(wei, dim=-1) # Softmaxは行の合計を1にし、平均の重みを回復する\nxbow3 = wei @ x\n# torch.allclose(xbow, xbow3) は True になるはず\n```\nなぜここで`softmax`を使うかというと、重み（`wei`）が固定された平均である必要はなく、重み自体が学習可能であったり、データに依存したりできるという重要なアイデアを導入するからです。これこそが、self-attentionが行うことです。\n\n## 位置情報の導入：Position Encoding\nSelf-Attentionメカニズム自体について詳しく見る前に、もう一つ重要な要素について触れておく必要があります。それは、トークンの位置に関する情報です。\n\nSelf-Attentionの基本的な計算（Query, Key, Valueを用いた加重集約）は、それ自体ではトークンがシーケンス内のどの位置にあるかを考慮しません。極端な話、単語の順番が入れ替わっても、各トークン間のAttentionスコアの計算自体は（入力ベクトルが同じであれば）変わりません。これでは、文の意味を正しく捉えることができません。「猫がマットの上に座った」と「マットが猫の上に座った」では意味が全く異なります。\n\nこの問題を解決するため、Transformerではトークン自体の意味を表す埋め込みベクトル（Token Embedding）に、そのトークンがシーケンス中のどの位置にあるかを示すPosition Encoding（位置エンコーディング）ベクトルを加算します。\n\nKarpathy氏の動画で実装されている`nanogpt`では、学習可能なPosition Encodingが用いられています。具体的には、`block_size`（扱える最大のシーケンス長）に対応する数の位置ベクトルを格納する埋め込みテーブル（`position_embedding_table`）を用意します。シーケンス長が`T`の場合、`0`から`T-1`までの整数をインデックスとして、対応する位置ベクトルをこのテーブルから取得します。\n```python\n# BigramLanguageModel内のforwardメソッドより抜粋\nB, T = idx.shape\n\n# idx and targets are both (B,T) tensor of integers\ntok_emb = self.token_embedding_table(idx) # (B,T,C) - トークン埋め込み\n# torch.arange(T, device=device) は 0 から T-1 までの整数のシーケンスを生成\npos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C) - 位置埋め込み\nx = tok_emb + pos_emb # (B,T,C) - トークン埋め込みと位置埋め込みを加算\nx = self.blocks(x) # ... このxがTransformerブロックへの入力となる ...\n```\nこのようにして、トークン自体の情報(`tok_emb`)とその位置情報(`pos_emb`)の両方を含んだベクトル`x`が作成されます。この`x`こそが、後続のTransformerブロック（Self-Attention層やFeedForward層）への実際の入力となるのです。これにより、モデルはトークンの意味だけでなく、その順序関係も考慮して処理を進めることができるようになります。\n\n\n## Self-Attention：データに基づいた情報の集約\n単純な平均化は、過去のすべてのトークンを平等に扱います。しかし、実際には、過去の一部のトークンが他のトークンよりもはるかに重要である場合があります。例えば、「The cat sat on the...」の次に続く単語を予測する場合、「The」よりも「cat」という単語の方が重要である可能性が高いです。\n\nSelf-attentionは、トークンが他のトークンに**問い合わせ（query）を行い、関連性に基づいて注意スコア（attention scores）**を割り当てることを可能にします。各トークンは3つのベクトルを生成します。\n\n1. Query (Q): 自分はどのような情報を探しているか？\n2. Key (K): 自分はどのような情報を持っているか？\n3. Value (V): もし自分に注意が向けられたら、どのような情報を提供するか？\n\nトークン`i`とトークン`j`間の注意スコア（またはaffinity）は、トークン`i`のQueryベクトル(`q_i`)とトークン`j`のKeyベクトル(`k_j`)の内積を取ることで計算されます。\n\n`affinity(i, j) = q_i ⋅ k_j`\n\n内積が大きい場合、QueryがKeyに良く一致していることを意味し、トークン`j`がトークン`i`にとって関連性が高いと判断されます。\n\n以下は、Attentionの単一の「Head」を実装する方法です。\n\n```python\n# version 4: self-attention!\ntorch.manual_seed(1337)\nB,T,C = 4,8,32 # batch, time, channels (埋め込み次元)\nx = torch.randn(B,T,C) # 入力トークンの埋め込み + 位置エンコーディング\n\n# 単一のHeadがself-attentionを実行する様子を見てみましょう\nhead_size = 16 # このHeadのK, Q, Vベクトルの次元\n# 入力'x'をK, Q, Vに射影するための線形層\nkey   = nn.Linear(C, head_size, bias=False)\nquery = nn.Linear(C, head_size, bias=False)\nvalue = nn.Linear(C, head_size, bias=False)\n\nk = key(x)   # (B, T, head_size)\nq = query(x) # (B, T, head_size)\n\n# 注意スコア（\"affinities\"）を計算\n# (B, T, head_size) @ (B, head_size, T) ---> (B, T, T)\nwei =  q @ k.transpose(-2, -1)\n\n# --- スケーリングステップ (後述) ---\nwei = wei * (head_size**-0.5) # アフィニティをスケーリング\n\n# --- Decoderのためのマスキング ---\ntril = torch.tril(torch.ones(T, T, device=x.device)) # xと同じデバイスを使用\nwei = wei.masked_fill(tril == 0, float('-inf')) # 未来のトークンをマスク\n\n# --- スコアを正規化して確率を取得 ---\nwei = F.softmax(wei, dim=-1) # (B, T, T)\n\n# --- Valueの重み付き集約を実行 ---\nv = value(x) # (B, T, head_size)\n# (B, T, T) @ (B, T, head_size) ---> (B, T, head_size)\nout = wei @ v\n\n# out.shape は (B, T, head_size)\n```\n\n重要なステップを分解してみましょう。\n\n1. 射影（Projection）: 入力`x`（トークン埋め込みと位置エンコーディングを含む）が、線形層によってK、Q、V空間に射影されます。\n2. アフィニティ計算（Affinity Calculation）: `q @ k.transpose(...)` は、バッチ内の各シーケンスにおける全てのQueryベクトルとKeyベクトルのペアの内積を計算します。これにより、生の注意スコアである`wei`（形状 `B, T, T`）が得られます。\n3. スケーリング（Scaling）: スコア`wei`は`head_size`の平方根でスケールダウンされます。これは、特に初期化段階での学習を安定させるために重要です。スケーリングがないと、内積の分散がhead_sizeと共に増加し、`softmax`の入力が勾配の非常に小さい領域に押しやられ、学習が妨げられる可能性があります。\n4. マスキング（Masking (Decoder固有)）: GPTのような自己回帰型（autoregressive）言語モデリングでは、位置`t`のトークンは位置`t`までのトークンにのみ注意を向けるべきです。これは、未来の位置（`j > t`）に対応する注意スコアを下三角行列（`tril`）を用いた`masked_fill`で負の無限大に設定することで実現されます。これにより、`softmax`は未来のトークンにゼロの確率を割り当てます。（BERTのようなEncoderブロックでは、この causal mask は使用されません。）\n5. Softmax: マスクされたスコアに対して行ごとに`softmax`を適用します。これにより、スコアは各トークン`t`について合計が1になる確率に変換され、先行するトークン`0`から`t`までの注意分布を表します。\n6. Valueの集約（Value Aggregation）: 各トークン`t`の最終出力`out`は、`wei`内の注意確率によって重み付けされた、全トークンのValueベクトル（`v`）の重み付き合計です。`out = wei @ v`。\n\n出力`out`（形状 `B, T, head_size`）は、学習されたK、Q、Vの射影に基づいて、シーケンス内の他の関連トークンから集約された情報を各トークンごとに含んでいます。\n\n## Multi-Head Attention：多角的な視点\n単一のAttention Headは、ある特定タイプの関係性（例：名詞と動詞の一致）に焦点を当てるかもしれません。多様な関係性を捉えるために、Transformerは**Multi-Head Attention**を使用します。\n\n```python\nclass Head(nn.Module):\n    \"\"\" self-attentionの単一ヘッド \"\"\"\n    def __init__(self, head_size):\n        super().__init__()\n        self.key = nn.Linear(n_embd, head_size, bias=False)\n        self.query = nn.Linear(n_embd, head_size, bias=False)\n        self.value = nn.Linear(n_embd, head_size, bias=False)\n        # trilをバッファとして登録（パラメータではない）\n        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n        self.dropout = nn.Dropout(dropout) # Dropoutを追加\n\n    def forward(self, x):\n        B,T,C = x.shape\n        k = self.key(x)   # (B,T,head_size)\n        q = self.query(x) # (B,T,head_size)\n        # 注意スコア（\"affinities\"）を計算\n        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # head_sizeでスケーリング\n        # Tに基づいて動的にマスクを適用\n        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n        wei = F.softmax(wei, dim=-1)\n        wei = self.dropout(wei) # 注意の重みにDropoutを適用\n        # Valueの重み付き集約を実行\n        v = self.value(x) # (B,T,head_size)\n        out = wei @ v\n        return out\n\nclass MultiHeadAttention(nn.Module):\n    \"\"\" self-attentionの複数ヘッドを並列に実行 \"\"\"\n    def __init__(self, num_heads, head_size):\n        super().__init__()\n        # 複数のHeadインスタンスを作成\n        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n        # 連結後の射影層\n        self.proj = nn.Linear(num_heads * head_size, n_embd) # n_embd = num_heads * head_size\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        # 各ヘッドを並列に実行し、結果をチャネル次元で連結\n        out = torch.cat([h(x) for h in self.heads], dim=-1) # (B, T, num_heads * head_size)\n        # 連結された出力を元のn_embd次元に再射影\n        out = self.dropout(self.proj(out)) # (B, T, n_embd)\n        return out\n```\nこれは単純に複数のHeadモジュールを並列に実行し、それぞれが異なる学習済みK、Q、V射影を持つ可能性があります。各ヘッドの出力（それぞれ `B, T, head_size`）は連結され（`B, T, num_heads * head_size`）、その後、別の線形層（`self.proj`）を用いて元の埋め込み次元（`B, T, n_embd`）に再射影されます。これにより、モデルは異なる表現部分空間からの情報に同時に注意を向けることができます。\n\n## Attentionの応用：Self-Attention, Cross-Attention, Encoder/Decoderブロック\nこれまで解説してきたAttentionの基本的な仕組みは、**Self-Attention**と呼ばれるものでした。これはQuery(Q), Key(K), Value(V)のベクトルがすべて同じ入力シーケンス（`x`）から生成され、シーケンス内のトークンが相互に注意を向け合うものでした。しかし、このSelf-Attentionの使われ方や、Attentionメカニズム全体にはいくつかの重要なバリエーションが存在します。\n\nまず、Self-Attention自体の使われ方によって、それがEncoderブロックの一部として機能するのか、Decoderブロックの一部として機能するのかが変わってきます。この違いを生む主な要因は、Attentionスコア計算におけるマスキングの有無です。\n\nDecoderブロックで使われるSelf-Attentionでは、未来の情報を参照しないようにするための因果マスキング（causal masking）、つまり三角マスクが適用されます。これは、GPTのような自己回帰（autoregressive）モデルや、機械翻訳のデコーダー部分のように、過去の情報のみに基づいて次のトークンを生成する必要があるタスクで不可欠です。Karpathy氏の動画で構築された`nanogpt`は、まさしくこのDecoderブロックのみで構成されるモデルです。\n\n一方、Encoderブロックで使われるSelf-Attentionでは、この因果マスキングは適用されません。シーケンス内のすべてのトークンが、他のすべてのトークン（過去も未来も含む）に自由に注意を向けることができます。これは、BERTのように入力テキスト全体の文脈理解を目的とするモデルや、機械翻訳におけるエンコーダー部分（入力文全体の情報を符号化する役割）などで用いられます。入力シーケンス全体の双方向の文脈を捉えるのに適しています。\n\n次に、Attentionメカニズムのもう一つの重要な形態が**Cross-Attention**です。これはSelf-Attention（マスキングの有無に関わらず）とは異なり、Query、Key、Valueの由来が異なります。Cross-Attentionでは、Query(Q)はあるソース（例えばデコーダー側の状態）から生成されますが、Key(K)とValue(V)は別のソース（例えばエンコーダーの最終出力）から提供されます。\n\nこのCross-Attentionは、主にEncoder-Decoderアーキテクチャにおいて、EncoderとDecoderを接続する役割を果たします。デコーダーが出力トークンを生成する際に、Cross-Attentionを通じてエンコーダーが符号化した入力情報全体を常に参照できるようにします。機械翻訳タスクで、翻訳先の言語を生成しながら常に翻訳元の文章の意味を考慮する、といったことを可能にするメカニズムです。\n\n`nanogpt`のようなdecoder-onlyモデルでは、外部の入力シーケンスを処理するEncoder部分が存在しないため、EncoderブロックやCross-Attentionは必要なく、因果マスキングを用いたSelf-Attention（Decoderブロック）のみで構成されている、というわけです。\n\n## Transformerブロック：通信と計算\nAttentionは通信メカニズムを提供します。しかし、モデルは集約された情報を処理するための計算も必要です。標準的なTransformerブロックは、Multi-Head Self-Attentionと、単純な位置ごとのFeedForwardネットワークを組み合わせます。\n\n重要な点として、各サブレイヤー（AttentionとFeedForward）の周囲に**Residual Connections（残差接続）とLayer Normalization（層正規化）**が追加されます。\n\n- **Residual Connections**: `x = x + sublayer(norm(x))`。サブレイヤーの入力`x`が、サブレイヤーの出力に加算されます。これにより、深いネットワークでの逆伝播時に勾配が流れやすくなり、学習の安定性と性能が大幅に向上します。\n- **Layer Normalization**: 各トークンについて、特徴量をチャネル次元にわたって独立に正規化します。Batch Normalizationとは異なり、バッチ統計に依存しないため、シーケンスデータに適しています。これも学習を安定させます。Karpathy氏は、サブレイヤーの前にLayerNormを適用する一般的な「pre-norm」形式を実装しています。\n\n```python\nclass FeedFoward(nn.Module):\n    \"\"\" 単純な線形層と非線形活性化関数 \"\"\"\n    def __init__(self, n_embd):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(n_embd, 4 * n_embd), # 中間層は通常4倍大きい\n            nn.ReLU(),                    # ReLU活性化関数\n            nn.Linear(4 * n_embd, n_embd), # n_embdに再射影\n            nn.Dropout(dropout),           # 正則化のためのDropout\n        )\n\n    def forward(self, x):\n        return self.net(x)\n\nclass Block(nn.Module):\n    \"\"\" Transformerブロック：通信の後に計算 \"\"\"\n    def __init__(self, n_embd, n_head):\n        super().__init__()\n        head_size = n_embd // n_head\n        self.sa = MultiHeadAttention(n_head, head_size) # 通信 (Communication)\n        self.ffwd = FeedFoward(n_embd)                 # 計算 (Computation)\n        self.ln1 = nn.LayerNorm(n_embd)                # Attention前のLayerNorm\n        self.ln2 = nn.LayerNorm(n_embd)                # FeedForward前のLayerNorm\n\n    def forward(self, x):\n        # Pre-norm形式と残差接続\n        # LayerNorm適用 -> Self-Attention -> 残差を加算\n        x = x + self.sa(self.ln1(x))\n        # LayerNorm適用 -> FeedForward -> 残差を加算\n        x = x + self.ffwd(self.ln2(x))\n        return x\n```\n完全なGPTモデルは、これらのBlockレイヤーを複数、順番に積み重ねます。すべてのブロックを通過した後、最終的なLayerNormが適用され、その後、最終的なトークン表現を語彙サイズに射影する線形層が続き、次のトークンを予測するためのロジットが得られます。\n\n## 最終的なGPTモデルの構築\nこれまで解説してきたコンポーネントを統合し、最終的なGPTスタイルの言語モデル`GPTLanguageModel`を構築します。以下に示すコードは、Karpathy氏の動画における完成形であり、先に説明した`Block`（`MultiHeadAttention`と`FeedForward`を含む）などを組み合わせています。\n\n```python\n# (主要なハイパーパラメータを再掲)\n# hyperparameters\nbatch_size = 64 # 並列処理する独立したシーケンス数\nblock_size = 256 # 予測のための最大コンテキスト長\nmax_iters = 5000\neval_interval = 500\nlearning_rate = 3e-4\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\neval_iters = 200\nn_embd = 384     # 埋め込み次元数\nn_head = 6       # Attentionヘッドの数\nn_layer = 6      # Transformerブロックの層数\ndropout = 0.2    # ドロップアウト率\n# ------------\n\nclass GPTLanguageModel(nn.Module):\n\n    def __init__(self):\n        super().__init__()\n        # トークン埋め込みと位置埋め込みのテーブル\n        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n        # n_layer個のTransformerブロックを積み重ねる\n        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n        self.ln_f = nn.LayerNorm(n_embd) # 最終LayerNorm\n        self.lm_head = nn.Linear(n_embd, vocab_size) # 出力層（線形層）\n\n        # （動画本編では触れられていないが重要な）重み初期化\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        # （重み初期化の詳細は省略）\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx, targets=None):\n        B, T = idx.shape\n\n        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n        x = tok_emb + pos_emb # (B,T,C)\n        x = self.blocks(x) # (B,T,C) Transformerブロックを通過\n        x = self.ln_f(x) # (B,T,C) 最終LayerNormを適用\n        logits = self.lm_head(x) # (B,T,vocab_size) LMヘッドでロジットを計算\n\n        if targets is None:\n            loss = None\n        else:\n            # 損失計算のために形状を変更\n            B, T, C = logits.shape\n            logits = logits.view(B*T, C)\n            targets = targets.view(B*T)\n            loss = F.cross_entropy(logits, targets)\n\n        return logits, loss\n\n    def generate(self, idx, max_new_tokens):\n        # idxは現在の文脈におけるインデックスの(B, T)配列\n        for _ in range(max_new_tokens):\n            # Position Embeddingのサイズ制限のため、idxを最後のblock_sizeトークンに切り詰める\n            idx_cond = idx[:, -block_size:]\n            # 予測を取得\n            logits, loss = self(idx_cond) # forwardパスを実行\n            # 最後のタイムステップのみに注目\n            logits = logits[:, -1, :] # (B, C) になる\n            # softmaxを適用して確率を取得\n            probs = F.softmax(logits, dim=-1) # (B, C)\n            # 分布からサンプリング\n            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n            # サンプリングされたインデックスを実行中のシーケンスに追加\n            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n        return idx\n```\n\nこのGPTLanguageModelクラスでは、`__init__`メソッドで、これまで説明してきたトークン埋め込みと位置埋め込みテーブル(`token_embedding_table`, `position_embedding_table`)を定義した後、`n_layer`個の`Block`を`nn.Sequential`で積み重ねています。これがTransformerの中核部であり、入力ベクトルはここを通過することで段階的にリッチな表現へと変換されます。その後、最終的な`LayerNorm` (`ln_f`)を経て、出力用の線形層`lm_head`によって語彙数次元のロジットへと変換されます。また、安定した学習のための重み初期化メソッド`_init_weights`も含まれています。\n\n`forward`メソッドは、この一連の流れを実装しており、トークン埋め込みと位置埋め込みを加算したベクトルを`blocks`に通し、正規化と線形変換を経て最終的なロジットを出力します。\n\nテキスト生成を行う`generate`メソッドでは、自己回帰的にトークンを生成していきますが、ここで重要なのは`idx_cond = idx[:, -block_size:]`の部分です。位置埋め込みテーブル`position_embedding_table`のサイズが`block_size`に固定されているため、モデルに入力できるのは直近`block_size`個のトークンまでとなります。この制約のもとでforwardパスを実行し、最後のタイムステップのロジットから次のトークンをサンプリングし、シーケンスを伸長していく処理を繰り返します。\n\nコード全体を見ると、これらのモデル定義に加えて、学習を制御するハイパーパラメータ群（`batch_size`や`learning_rate`など）や、`AdamW`オプティマイザ、そして`estimate_loss`関数を用いた評価を含む標準的な学習ループが組み合わされていることがわかります。これらが一体となってGPTモデルの学習と推論を実現しています。\n\n## スケールアップと結果\nKarpathy氏は上の`GPTLanguageModel`（`n_layer=6, n_head=6, n_embd=384, dropout=0.2`）でTiny Shakespeareを学習させます。結果として得られるモデルは、はるかに一貫性のある（ただし、まだ意味をなさない）シェイクスピア風のテキストを生成し、十分なモデル容量と組み合わされたAttentionの力を示しています。\n\n```console\n# GPTLanguageModelからのサンプル出力\nFlY BOLINGLO:\nThem thrumply towiter arts the\nmuscue rike begatt the sea it\nWhat satell in rowers that some than othis Marrity.\n\nLUCENTVO:\nBut userman these that, where can is not diesty rege;\nWhat and see to not. But's eyes. What?\n```\n\nこのアーキテクチャ、すなわち**decoder-only Transformer**（causal maskを使用）は、基本的にGPT-2やGPT-3のようなモデルで使用されているものと同じですが、パラメータ数、層数、埋め込みサイズ、そして学習データ（シェイクスピアだけでなく膨大なインターネットテキスト）の点で、はるかに大規模になっています。\n\n## まとめ\nAttentionメカニズム、特にScaled dot-product self-attentionは、Transformerの能力を飛躍的に向上させた革新的な技術です。これにより、シーケンス内のトークンが動的にお互いを参照し、学習されたQuery-Keyの相互作用に基づいて関連性スコア（アフィニティ）を計算し、関連するトークンのValueベクトルからの情報を重み付きで集約することが可能になります。Multi-Head Attention、Residual Connections、Layer Normalization、そして位置ごとのFeedForwardネットワークと組み合わせることで、ChatGPTのようなAIに革命をもたらしているモデルの基本的な構成要素であるTransformerブロックが形成されます。\n\nKarpathy氏のように段階的に構築することで、強力でありながらも、その中心的なアイデアは把握可能であり、比較的簡潔なコードで実装できることがわかります。\n\n---\n\n*この記事は、Andrej Karpathy氏のYouTube動画「[Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY)」に基づいています。完全なコードやより深い洞察については、ぜひ動画と彼の[`nanogpt`](https://github.com/karpathy/ng-video-lecture/tree/master)リポジトリをご覧ください。*\n*この記事が、TransformerとAttentionの理解の一助となれば幸いです。*\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"markdown"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.23","theme":["cosmo","brand"],"title-block-banner":true,"title":"コードで理解するTransformer：AttentionとGPTモデル入門","author":"Junichiro Iwasawa","date":"2025-04-11","categories":["Machine Learning","Transformer","Python"],"image":"https://picsum.photos/id/83/200"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}