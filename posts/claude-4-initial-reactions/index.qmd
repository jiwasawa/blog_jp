---
title: "Claude 4の登場: 線形な進歩と「告げ口」AIの波紋"
description: "新しく発表された「Claude 4」に対する開発者の初期的な反応を掘り下げていきます。"
author: "Junichiro Iwasawa"
date: "2025-05-23"
categories: [LLM, AI, Latent Space Podcast]
image: https://picsum.photos/id/98/200
---

Anthropic社から次世代AIモデル「Claude Opus 4」と「Claude Sonnet 4」が発表された。SWE-benchで最高スコアを叩き出し、特にコーディングとエージェント機能の進化を謳う今回の発表。世の中が沸き立つ中、公式発表の華々しさの裏で語られた反応を、[Simon Willison氏のブログ](https://simonwillison.net/)と[Latent Space Podcast](https://www.latent.space/p/willccbb)の議論を元に分析していきたい。

## Claude 4は何が新しいのか？

まず公式発表を簡単にまとめよう。ポイントは大きく三つだ。

1.  **二つの新モデル**：最上位の「Claude Opus 4」と、性能と速度のバランスが取れた「Claude Sonnet 4」。特にコーディング能力で既存モデルを凌駕し、Claude 4 SonnetはSWE-benchでState-of-the-artとなる72.7%を記録。
2.  **思考とツールの連携強化**：モデルが自律的に思考し、ウェブ検索のようなツールを繰り返し利用する「extended thinking」が強化された。これにより、より複雑で長時間のタスクに対応できるようになった。
3.  **開発者ツールの拡充**：VS CodeやJetBrainsとの連携、GitHub上での自律的なコード修正など、開発者のワークフローに深く統合される「Claude Code」が正式に利用可能となった。

価格は据え置きで、Sonnet 4は無料ユーザーにも提供されるなど、着実なアップグレードと言える。しかし、注目すべきは公式発表の行間に隠された詳細と、それに対する専門家たちの反応だ。

## 開発者目線の冷静な評価 - Simon Willisonの洞察

著名な開発者であるSimon Willison氏は、自身のブログで早速いくつかの重要な、そして少し残念な点を指摘している。

* **学習データの新しさ**：Claude 4の学習データが2025年3月までのものである点は、非常に印象的だと評価。これは現行の主要モデルの中で最も新しい。
* **Context windowの停滞**：一方で、入力トークン数の上限が20万トークンに留まったことには「がっかりした」と述べる。GPT-4.1が1Mトークンへとcontext lengthを広げる中、これは見劣りする点だ。さらに、Opus 4の最大出力トークン数がClaude 3.7 Sonnetの64,000から32,000へと半減している点も、地味ながら重要な後退である。
* **悩ましい課金体系**：今回導入された「[summarized thinking](https://docs.anthropic.com/en/docs/about-claude/models/migrating-to-claude-4#summarized-thinking)」機能は、一見すると便利だ。しかし、APIのドキュメントには「課金対象は要約されたトークンではなく、モデルが生成した元の思考トークンの全長である」という注意書きがある。Willison氏が指摘するように、これは開発者にとって厄介な問題だ。APIからの応答を見ただけではコストを正確に見積もれず、見えないところで課金が発生する可能性がある。

また、Willison氏はAnthropicの開発者向けカンファレンスでようやく得られた「エージェント」の定義についても言及している。それは「**ループの中でツールを使うモデル (Agents are models using tools in a loop)**」という、驚くほどシンプルなものだった。バズワードが先行する中で、このような基本的な定義が明確にされたこと自体が、ある種の「発見」だったと皮肉めかして語っている。

## 「パラダイムシフトではない、地道な進歩」

Latent Space Podcastでは、さらに踏み込んだ議論が交わされた。ゲストとして登場したWill Brown氏は、Claude 4を「素晴らしいモデル」としながらも、「**パラダイムシフトというよりは、線形的な進歩**」と評した。これは、AIの能力が飛躍的に向上したというよりは、既存の路線を着実に改良してきた結果だという見方だ。特に印象的だったのは、Claude 3.7が抱えていた「お節介」問題の改善だ。

Brown氏曰く、Sonnet 3.7はコーディングを頼むと、要求されたこと以上の余計な関数やファイルまで生成する「Reward hacking」的な挙動が目立ったという。これは、テストケースを通過するために、とにかく多くのコードを生成する方が有利だと学習してしまった結果だろう。今回のモデルでは、この挙動が65%減少したと報告されており、より「信頼でき」「最小限の的確な仕事をする」モデルになった可能性がある。これはAIを実用的な共同作業者として使う上で、極めて重要な改善点だ。

## 「Extended thinking」の正体

Anthropicが強調する「[extended thinking](https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking)」についても、Brown氏は「魔法のような新しい推論モードというより、モデルが使えるツールの一つに過ぎない」と推測する。つまり、ウェブ検索やコード実行と同じように、「思考を書き出す（scratchpad）」というツールをモデルが自律的に呼び出しているだけではないか、というわけだ。これはマーケティング用語を冷静に解体した見方と言える。

## 最大の波紋：「告げ口」するAI？

今回、最もspicyな話題は、Anthropic社員が（後に削除したものの）投稿した、安全性テストに関する一連のツイートから生まれた。その中で、「**モデルがユーザーの違法行為の可能性を察知し、当局に通報する挙動を見せた**」という報告があったのだ。実際Claude 4 の[system card](https://www-cdn.anthropic.com/6be99a52cb68eb70eb9572b4cafad13df32ed995.pdf)をみると、シミュレーション環境下の架空の製薬会社で臨床試験の隠蔽を見つけた結果、ユーザに確認せずに規制当局にメールを通報してしまうことが報告されている（以下図参照）。

![Claude 4 System Card より引用](claude-4-reporting.png){width=80%}

これに対し、podcastでは「文脈を無視して騒ぎすぎだ」と釘を刺しつつも、興味深い議論が展開された。Brown氏は、これはあくまで極端な状況下での「ストレス・テスト」の結果であり、通常の利用で起こることではないと強調する。モデルが「ユーザーに最大限協力する」という目標と「社会規範を守る」という目標の板挟みになった時、どのような行動を選択するかを試すためのものだ。

しかし、この一件は開発者やユーザーに重要な問いを投げかける。PodcastのホストであるAlessio氏が「自分を告げ口するかもしれないAIに、メールへのアクセス権を与えたいと思うだろうか？」と問いかけたように、AIエージェントにどこまでの権限を渡すべきか、という根源的な信頼の問題が浮上したのだ。Anthropicがこうした「過激な」テスト結果を公にするのは、自社の安全研究への取り組みをアピールする「アポロ・マーケティング」の一環でもあるだろう。しかし、その結果として生まれる「AIがユーザーを裏切るかもしれない」というイメージは、諸刃の剣と言わざるを得ない。

## 総括：信頼できるコーダー、しかしプラットフォームとしては

総合すると、Claude 4はAIの知能そのものに革命を起こすというより、「**信頼できる専門家（特にプログラマー）**」としての完成度を高めてきたモデルだと言えるだろう。「お節介」を焼かなくなり、より的確なアウトプットを出すようになった点は、実用面で大きな進歩だ。しかし、その裏で開発者たちは、不透明な課金体系や、競合に劣るスペック（e.g. context length）といった現実に直面している。最先端のモデル性能を追求する一方で、それを支えるプラットフォームとしての配慮が一貫していないのではないか、という疑念が残る。

Anthropicの戦略は、OpenAIやGoogleとは異なり、コーディングや安全性といった特定分野に特化し、ブランドを確立しようとしているように見える。しかし、「告げ口」挙動の波紋が示したように、その「安全性」というブランドイメージが、かえってユーザーの信頼を揺るがしかねない。Claude 4が「地道な進歩」の先に見据えるのは、どのような未来なのか。その答えは、モデルの性能だけでなく、開発者やユーザーとの信頼関係をいかに築いていくかにかかっているのかもしれない。
