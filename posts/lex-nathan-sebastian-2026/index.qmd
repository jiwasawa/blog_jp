---
title: "State of AI 2026: Post-trainingの覇権と「AGIの夢」の変質"
description: "2026年のAI開発は、大規模なPre-training競争からPost-trainingの効率化・洗練へとシフトし、「AGIの夢」は特化型モデル群の連携という現実解へと変質していく。"
author: "Junichiro Iwasawa"
date: "2026-02-08"
categories: [LLM, AI, Podcast, Lex Fridman Podcast]
image: https://picsum.photos/id/162/200
---

[Lex Fridman PodcastにおけるNathan LambertとSebastian Raschkaの対談](https://youtu.be/EV7WhVT270Q?si=EvcbAfYyoU1Dhbcu)は、2026年初頭におけるAI業界の空気感を鮮明に切り取っている。

2025年の「DeepSeekショック」から1年。かつてのような、単にパラメータ数を増やせば魔法のように性能が上がるという牧歌的なScaling Lawsの時代は終わりを告げた。議論の主戦場はPre-training（事前学習）の巨大化競争から、いかに効率よく推論させるか、そしていかにPost-training（事後学習）でモデルを「躾ける」かという、よりエンジニアリング的かつ泥臭い領域へとシフトしている。

本稿では、彼らの対談内容をベースに、現在のAI開発の最前線で何が起きているのか、そして「AGI（汎用人工知能）」という言葉の定義がどのように変質しつつあるのかを分析する。

## Pre-trainingの飽和とPost-trainingの台頭

2026年のAI開発における最大のトピックは、間違いなくPost-trainingへの重心移動である。

Nathan Lambertが指摘するように、Pre-trainingにおけるScaling Laws（計算量と性能の比例則）自体は死んでいないものの、そのコスト対効果は明らかに鈍化している。数兆トークンを学習させるコストが天文学的に跳ね上がる中で、OpenAIやGoogle、Anthropicといったプレイヤーが見出した活路が、**RLVR (Reinforcement Learning with Verifiable Rewards)** である。

これまでのRLHF（人間によるフィードバックを用いた強化学習）が、人間の「好み」や「バイアス」を学習させる、いわば「文体やトーンの矯正」であったのに対し、RLVRは数学やコーディングといった「正解（Verifiable Rewards）」が存在する領域において、モデル自身に試行錯誤（Trial and Error）を繰り返させる手法だ。DeepSeek R1が示したように、このプロセスを通じてモデルは内部的な「Aha moment（気づき）」を得て、飛躍的に推論能力を向上させる。

しかし、ここには落とし穴もある。Sebastian Raschkaが警鐘を鳴らすように、RLVRによる性能向上の一部は、ベンチマークデータへの過学習（Data Contamination）である可能性が排除できない。特定の数学問題のパターンを暗記しているだけなのか、真に論理的思考を獲得したのか。この境界線は、我々が考える以上に曖昧だ。推論時計算（Inference-time compute）を増やすことで、モデルは見かけ上のIQを高めることができるが、それは「賢さ」の本質的向上なのか、単なる「思考時間の暴力」なのかという議論は残る。

## 「AGIの夢」の終焉とエージェントの現実解

対談の中で最も挑発的かつ核心を突いていたのは、Lambertの「The dream is dying（夢は死につつある）」という発言だろう。

ここでの「夢」とは、一つの巨大なモデルがあらゆるタスクを完璧にこなすという、かつてのシリコンバレーが描いた「God Model」としてのAGIである。2026年の現実は、万能の神ではなく、特定のタスクに特化した「Jagged（ギザギザした）」能力を持つモデル群の連携へと向かっている。

コーディングにはClaude Opus 4.5やClaude Codeが、検索や情報の整理にはGeminiやChatGPTが、そしてローカルでの軽量な推論にはLlamaやDeepSeekの派生モデルが使われる。ユーザーは無意識のうちに、自分の目的に合わせて最適な「道具」を使い分けるようになっている。これはAIが「人格を持った存在」から「高度なSaaS」へと着地したことを意味する。

また、Robotics（ロボット工学）の分野においても、End-to-Endですべてを学習するモデルよりも、World Models（世界モデル）やシミュレーション環境での学習を組み合わせたアプローチが現実的であることが見えてきた。汎用的な知能が身体を持って歩き回る未来よりも先に、倉庫内での物流最適化や、スクリーンの中での「Computer Use（PC操作代行）」といった、限定的だが経済的インパクトの大きい自動化が進むだろう。

## Open Weightと地政学的力学

中国発のDeepSeekやQwenといったOpen Weightモデルの躍進は、米国のAI覇権に対するアンチテーゼとして機能している。

技術的な「アイデア」自体は瞬く間に国境を越え、コモディティ化する。MoE (Mixture of Experts) やLatent Attentionといったアーキテクチャの工夫は、論文が出た翌週には世界中のハッカーが再現実装を行う。Raschkaが指摘するように、長期的には技術そのものではなく、それを実行するための「Compute（計算資源）」と「データ」、そして組織文化が差別化要因となる。

NVIDIAのGPU支配は依然として盤石だが、推論専用チップへのシフトや、推論と学習の分離が進む中で、ハードウェアの勢力図も徐々に変化の兆しを見せている。米国政府が推進する「Adam Project」のようなOpen Model支援の動きは、AI開発が純粋な技術競争から、国家間のインフラ競争へとフェーズが移行したことを示唆している。

## 人間の役割：Agencyの復権

AIがコードを書き、メールを返し、論文の要約を行う世界において、人間は何をすべきか。

RaschkaとLambertのアドバイスは一貫している。「消費者（Consumer）になるな、構築者（Builder）であれ」ということだ。AIが生成する「Slop（低品質な生成物）」がインターネットを埋め尽くす中で、ただ漫然とAIを使うだけの人間は、アルゴリズムの波に飲まれていく。

プログラミングにおいて、AIにコードを書かせることは効率的だが、Raschkaが強調するように「ゼロから作る（Build from scratch）」経験なしに、AIが出力したコードの良し悪しを判断することはできない。AIを「ペアプログラマー」として使いこなしつつも、最終的な意思決定と責任を持つ「Agency（主体性）」を維持すること。これこそが、AIに代替されないための唯一の防衛策であり、同時にAI時代におけるエンジニアの楽しみ方（Joy）でもある。

2026年、AIは魔法ではなくなった。それは強力だが癖のある「道具」であり、それをどう使いこなすかというボールは、完全に我々人間に投げ返されている。
