---
title: "GRPO++：LLMの強化学習を成功に導くための高度なテクニックと実践的トリック"
description: "Cameron R. Wolfeによるブログ記事を基に、LLMの推論能力を向上させるGroup Relative Policy Optimization (GRPO)の限界と、DAPO、Dr. GRPO、TISなどの最新技術による改善手法を解説する。"
author: "Junichiro Iwasawa"
date: "2026-01-08"
categories: [LLM, AI]
image: https://picsum.photos/id/143/200
---

NetflixのシニアリサーチサイエンティストであるCameron R. Wolfe氏による優れたブログ記事 [GRPO Tricks](https://cameronrwolfe.substack.com/p/grpo-tricks) を基に、Large Language Model (LLM) の推論能力を飛躍させる強化学習（RL）の最新動向と、Group Relative Policy Optimization (GRPO) の実践的な改善手法について解説する。

[DeepSeek-R1](https://arxiv.org/abs/2501.12948) の登場以来、推論モデル（Reasoning Models）の開発において強化学習は中心的な役割を果たしている。その核心にあるのがGRPOアルゴリズムだ。概念的にはシンプルで計算効率に優れたGRPOだが、大規模な学習において「素朴な（Vanilla）」GRPOをそのまま適用すると、学習の不安定化や予期せぬ挙動に直面することが多い。

本稿では、GRPOが抱える本質的な課題と、それを克服するために提案された「DAPO」「Dr. GRPO」「TIS」といった最新のテクニックを詳細に掘り下げる。

## GRPOの基礎と「Vanilla」版の限界

詳細なテクニックに入る前に、基本をおさらいしておこう。近年のLLMにおける強化学習は、主にReinforcement Learning from Human Feedback (RLHF) か、Reinforcement Learning with Verifiable Rewards (RLVR) のいずれかのアプローチをとる。特に数学やコーディングといった「正解」が検証可能な領域（RLVR）において、GRPOは標準的な選択肢となっている。

### PPOからGRPOへ

GRPOは、Proximal Policy Optimization (PPO) の進化系と位置付けられる。PPOは、ポリシーの更新幅を制限するクリッピング機構と、価値関数（Critic）を用いたアドバンテージ推定により学習を安定化させてきた。

対してGRPOは、**Criticモデル（価値関数）を排除**することで計算コストを大幅に削減する。代わりに、同一のプロンプトに対して複数の出力をサンプリングし（これを「グループ」と呼ぶ）、グループ内での相対的な報酬に基づいてベースラインを推定する。

### Vanilla GRPOが直面する壁

DeepSeek-R1の成功を受けて多くの研究者がGRPOの再現を試みたが、単純な実装（Vanilla GRPO）では以下のような問題が頻発した。

1.  **学習の不安定性とノイズ:** 報酬曲線が安定せず、激しく変動する。
2.  **エントロピーの崩壊（Entropy Collapse）:** モデルの出力分布が決定的になりすぎ、探索能力が失われる。
3.  **過剰な応答長:** 不正解であっても、とにかく長く出力すれば損失が下がるという抜け道をモデルが学習してしまう。
4.  **低いサンプル効率:** 学習の収束に膨大なイテレーションを要する。

これらの問題は、オリジナルの論文では語られなかった「実装上の詳細（Tricks）」が極めて重要であることを示唆している。

## GRPOを進化させる主要なイノベーション

これらの課題に対処するため、DAPO、Dr. GRPO、TISといった改良手法が提案されている。これらは単なるパラメータ調整ではなく、目的関数やシステム設計の根本的な見直しを含んでいる。

### 1. DAPO: 探索と効率の最適化

[[Yu et al., 2025]](https://arxiv.org/abs/2503.14476) で提案された **DAPO (Decoupled Clip and Dynamic Sampling Policy Optimization)** は、以下の4つの重要な変更を加えることで、GRPOのパフォーマンスを劇的に向上させた。

*   **Clip Higher（クリッピング上限の緩和）:**
    PPO/GRPOではポリシー比（Policy Ratio）を $[1-\epsilon, 1+\epsilon]$ の範囲にクリッピングする。しかし、DAPOの研究では、この上限（$1+\epsilon$）が、確率の低いトークン（探索的トークン）の採用を不当に阻害していることが判明した。そこで、上限側のクリッピング幅を広げる（例：$\epsilon=0.2$から$0.28$へ）ことで、探索を促進し、エントロピーの崩壊を防ぐ手法が提案された。

*   **Dynamic Sampling（動的サンプリング）:**
    学習が進むと、グループ内のすべての出力が正解（報酬が全て1）になるケースが増える。GRPOは相対評価であるため、全員が正解だとアドバンテージがゼロになり、勾配が発生しない（学習が進まない）。DAPOでは、このような「完全に解けてしまったプロンプト」を動的にフィルタリングし、常に学習効果のある難易度の高いサンプルを供給し続けることで、サンプル効率を向上させる。

*   **Token-Level Loss（トークンレベル損失）:**
    Vanilla GRPOでは、シーケンス全体の平均損失を用いていたが、これは長い応答に対してトークンあたりの寄与を薄めてしまうバイアスを生む。DAPOは損失をトークンレベルで集計し直すことで、応答の長さに依存しない公平な学習を実現した。

*   **Overlong Reward Shaping（長大出力への対処）:**
    最大トークン数で打ち切られた（Truncated）サンプルに対して、一律に負の報酬を与えるだけでは学習が不安定になる。DAPOでは、打ち切られたサンプルをマスク（無視）するか、長さに基づいてソフトなペナルティを与えることで、モデルが「適切な長さ」を学習できるよう誘導する。

### 2. Dr. GRPO: バイアスの是正

[[Liu et al., 2025]](https://arxiv.org/abs/2503.20783) による **GRPO Done Right (Dr. GRPO)** は、GRPOの数式に潜む統計的なバイアスを指摘し、修正案を提示した。

*   **正規化定数の固定:**
    通常のGRPOは、シーケンスの損失をそのシーケンスのトークン数で割って正規化する。しかし、これは「長く喋れば喋るほど、分母が大きくなり損失が小さくなる」という誤ったインセンティブ（Length Bias）をモデルに与えてしまう。Dr. GRPOでは、トークン数ではなく**固定定数**で正規化を行うことで、このバイアスを排除した。これにより、無駄に長いだけの不正解な回答が減少し、真に必要な推論のみが行われるようになる。

*   **アドバンテージ計算における標準偏差の削除:**
    GRPOのアドバンテージ計算では、グループ報酬の標準偏差で除算を行う。しかし、問題が極端に簡単（全員正解）または極端に難しい（全員不正解）場合、標準偏差が極小になり、アドバンテージが爆発的に大きくなる不安定性を招く。Dr. GRPOではこの標準偏差項を削除し、単に平均との差分のみを用いることで学習を安定化させた。

### 3. TIS: エンジニアリングギャップの解消

[[Yao et al., 2025]](https://fengyao.notion.site/off-policy-rl) は、**Truncated Importance Sampling (TIS)** を用いて、システム実装上の課題に切り込んだ。

現代のRL学習フレームワークでは、推論（Rollout）を行うエンジン（例：vLLM）と、学習（Policy Update）を行うエンジン（例：FSDP/Megatron）が分離していることが多い。たとえ同じ重みを持っていても、これら二つのエンジンが出力する確率は、浮動小数点の精度やカーネル実装の違いにより微妙に異なる（Engine Gap）。

この差異は、本来「On-Policy」であるはずの学習を、意図せず「Off-Policy」にしてしまい、学習効率を低下させる。TISは、重要度サンプリング（Importance Sampling）の考え方を導入し、推論エンジンと学習エンジンの確率比を用いて勾配を補正する。これにより、システムレベルの不一致を数学的に吸収し、真にOn-Policyな学習を保証する。

### 4. その他の派生形：GSPOとGMPO

*   **[GSPO (Group Sequence Policy Optimization)](https://arxiv.org/abs/2507.18071):**
    Mixture-of-Experts (MoE) モデルにおいて、トークンレベルの重要度比率を使用すると、エキスパートの選択が不安定になる問題がある。GSPOは重要度比率をシーケンスレベルで計算することで、特にQwen 3のような大規模MoEモデルのRL学習を安定化させることに成功している。

*   **[GMPO (Geometric Mean Policy Optimization)](https://arxiv.org/abs/2507.20673):**
    外れ値となる極端な重要度比率による不安定性を防ぐため、算術平均の代わりに**幾何平均**を用いてトークンレベルの損失を集約する手法。これにより、数学的推論タスクにおいて一貫した性能向上が確認されている。

## 結論：最先端のRLパイプラインの構築に向けて

GRPOは強力なアルゴリズムだが、それを大規模かつ安定して動作させるためには、上記のような数多くの工夫が必要となる。

実際、最新の **[Olmo 3](https://arxiv.org/abs/2512.13961)** モデルの学習パイプラインでは、これらのテクニックが複合的に採用されている。具体的には、ゼロ勾配サンプルのフィルタリング（DAPO）、トークンレベル損失、KLペナルティの削除、クリッピング上限の緩和、TISによるエンジン間ギャップの補正、そしてアドバンテージ計算からの標準偏差の削除（Dr. GRPO）などが組み込まれている。

LLMの推論能力を高めるための強化学習（RL）は、単なるアルゴリズムの適用から、複雑なシステムエンジニアリングの領域へと進化している。今後も新たな論文や実装上のトリック（秘伝のタレ）が次々と公開されるだろう。我々実践者は、理論的な理解と実験的な検証の両輪で、最適な学習レシピを模索し続ける必要がある。

## 参考文献

1. Yu, Qiying, et al. "[Dapo: An open-source llm reinforcement learning system at scale.](https://arxiv.org/abs/2503.14476)" arXiv preprint arXiv:2503.14476 (2025).
2. Liu, Zichen, et al. "[Understanding r1-zero-like training: A critical perspective.](https://arxiv.org/abs/2503.20783)" arXiv preprint arXiv:2503.20783 (2025).
3. F. Yao, et al. "[Your efficient rl framework secretly brings you off-policy rl training.](https://fengyao.notion.site/off-policy-rl)" (2025).
4. Zheng, Chujie, et al. "[Group sequence policy optimization.](https://arxiv.org/abs/2507.18071)" arXiv preprint arXiv:2507.18071 (2025).
5. Zhao, Yuzhong, et al. "[Geometric-mean policy optimization.](https://arxiv.org/abs/2507.20673)" arXiv preprint arXiv:2507.20673 (2025).
