---
title: "Terminal-Bench 2.0：CLI環境におけるAIエージェントの「真の実力」を測る新たな指標"
description: "Terminal-Bench 2.0は、CLI環境におけるAIエージェントの複雑で現実的なタスク遂行能力を測る新たなベンチマークであり、最新モデルの性能と課題を浮き彫りにする。"
author: "Junichiro Iwasawa"
date: "2026-01-27"
categories: [LLM, AI]
image: https://picsum.photos/id/155/200
---

近年、LLM（Large Language Model）を核としたAIエージェントの開発競争は熾烈を極めている。特に、チャットボットのような単純な対話を超え、現実世界の複雑なタスクを自律的に遂行する能力が求められている。その最前線となるのが、開発者やシステム管理者の主戦場であるCLI（Command-Line Interface）環境だ。

本記事では、AIエージェントの能力を厳密に評価するために新しく提案されたベンチマーク**「[Terminal-Bench 2.0](https://tbench.ai)」**について解説する。既存のベンチマークが抱える課題を克服し、フロンティアモデルですら苦戦するこの「難解かつ現実的」なデータセットは、AIエージェント研究の新たな羅針盤となり得るのか。その全貌と、初期実験から見えてきた最新モデルの傾向を紐解いていく。

## Terminal-Bench 2.0とは何か？

Terminal-Bench 2.0は、AIエージェントがLinuxターミナル環境を用いて、どれだけ現実に即した複雑なタスクを完遂できるかを測定するために設計された包括的なベンチマークである。

従来のベンチマーク（例：HumanEvalやSWE-benchの簡易版など）は、比較的短期間で解決可能なタスクや、人工的に簡略化された環境に依存する傾向があった。しかし、実際のエンジニアリング業務は、環境設定、デバッグ、ライブラリの依存関係解消、そして長時間にわたる試行錯誤を伴うものである。

Terminal-Bench 2.0は、こうしたギャップを埋めるために以下の特徴を備えている。

1.  **現実的なタスク設定（Realistic Tasks）：** 実際の専門的なワークフローから着想を得た89のタスクで構成されている。レガシーシステムの構成変更、研究論文の実装再現、セキュリティ脆弱性の修正など、深いドメイン知識と自律的な問題解決能力が要求される。
2.  **包括的な評価環境（Comprehensive Evaluation）：** 各タスクはDockerコンテナとして完全に隔離・パッケージ化されている。エージェントにはタスク記述（Instruction）が与えられ、最終的なコンテナの状態に対して一連のテストスイートが実行されることで、正否が判定される。
3.  **高い難易度（Focus on Difficulty）：** 意図的に難易度が高く設定されており、現在の最先端（Frontier）モデルであっても、平均スコアは65%を下回る。これにより、将来的なモデルの進歩を長期的に計測することが可能となる。

データセットおよび評価用ハーネスは、[tbench.ai](https://tbench.ai)にて公開されており、コミュニティによる検証と発展が期待されている。

## ベンチマークの構成と「Terminus 2」

Terminal-Benchの評価プロセスにおける公平性を担保するために導入されたのが、**「Terminus 2」**と呼ばれるエージェントスカフォールド（Agent Scaffold）である。

多くの商用エージェント（Claude CodeやGitHub Copilot CLIなど）は、モデルの性能だけでなく、エージェント自体の実装（プロンプトエンジニアリングやツール使用の最適化）に強く依存している。純粋なモデルごとの性能差を比較するために、Terminus 2は極めてシンプルな構造を採用している。

*   **単一ツール:** 使用できるのは「ヘッドレスなターミナル」のみ。
*   **操作方法:** 純粋なBashコマンドの実行のみでタスクを遂行する（ファイル編集も`sed`や`vim`などのコマンド経由で行う）。

この「素の」環境により、モデルが本来持っている推論能力やLinuxコマンドへの習熟度が浮き彫りになる。

## 最新AIモデルの性能評価

Terminal-Bench 2.0を用いた初期評価の結果は、現在のAI技術の到達点と限界を明確に示している。

### 1. 全体的な解決率（Resolution Rate）
最も高いスコアを記録した組み合わせは、**GPT-5.2**をバックエンドに用いた**Codex CLI**エージェントであり、解決率は**63%**であった。これに**Claude Opus 4.5**や**Gemini 3 Pro**（いずれもTerminus 2を使用）が50%後半台で続く。
逆に言えば、人類最高峰のAIモデルを使用しても、提示されたタスクの約4割は解決できていない。これは、単純なコード生成とは異なり、環境の状態を把握しながら長期間にわたって作業を継続することの難しさを物語っている。

### 2. プロプライエタリ vs オープンウェイト
明確な傾向として、GPT、Claude、Geminiといったプロプライエタリ（クローズド）なモデルが上位を独占している。オープンウェイトモデル（LLaMAやQwenなど）も健闘してはいるものの、Terminus 2と組み合わせた場合の最高スコアは36%程度（Kimi K2 Thinking）に留まり、トップ層とは依然として大きな差が開いている。

### 3. モデル選択の重要性
興味深い知見として、**「エージェントのスカフォールド（枠組み）よりも、使用する基盤モデルの性能の方が結果に大きく影響する」**という点が挙げられる。
例えば、同じCodex CLIエージェントを使用した場合でも、モデルをGPT-5.2からGPT-5-Nanoに変更すると性能は劇的に低下する。一方で、強力なモデルを使えば、シンプルなTerminus 2スカフォールドでも、高度に最適化されたエージェント（OpenHandsなど）と同等以上の成果を出すケースが見られた。

### 4. 進化の速度
モデルのリリース時期とベンチマークスコアには強い相関が見られ、新しいモデルほど高スコアを記録している。この分野におけるAIの能力は急速に向上しており、現在は「難関」とされるタスクも、近い将来には解決率が飽和する可能性が示唆されている。

## エラー分析：なぜエージェントは失敗するのか？

Terminal-Bench 2.0の研究チームは、エージェントの失敗パターン（Failure Modes）について詳細な分析を行っている。失敗は主に以下のカテゴリーに分類される。

### 実行エラー（Execution Errors）
最も頻繁に見られるのが実行エラーである。これはエージェントが指示（Instruction）に忠実に従わない、あるいは仕様を無視した行動をとるケースを指す。
*   **指示違反:** 「特定のファイルパスに出力せよ」という指示を無視する。
*   **ループ:** 同じエラーを何度も繰り返す（Step repetition）。

### 整合性と検証の欠如（Coherence and Verification Errors）
エージェントが「タスク完了」を宣言したにもかかわらず、実際には要件を満たしていないケースも多い。
*   **幻覚（Hallucination）:** テストが通っていないのに「成功しました」と報告する。
*   **検証不足:** 変更を加えた後に、それが正しく動作しているか（例：コンパイルが通るか、スクリプトが動くか）を確認せずに終了してしまう。

### コマンドレベルの失敗
より低レイヤーな視点では、以下のような初歩的なミスが依然として多いことが判明した。
*   **Command not found:** 存在しないコマンドや、インストールされていないツールを呼び出そうとする。
*   **File not found:** パス指定のミスや、ファイル生成の失敗。

これらのエラーは、現在のLLMが「文脈を保持し続ける能力（Long-context reasoning）」や「自己批判・自己修正する能力（Self-correction）」において、まだ改善の余地があることを示唆している。

## まとめと今後の展望

Terminal-Bench 2.0は、AIエージェントにとっての「現実の壁」を可視化した。63%という最高スコアは、AIが実用的なレベルに近づいていることを示す一方で、人間のエンジニアが日々行っている複雑なトラブルシューティングやシステム構築を完全に自律化するには、まだ距離があることも示している。

今後の研究開発においては、単にモデルのパラメータ数を増やすだけでなく、以下の点が重要になるだろう。

1.  **指示遵守能力の向上:** 複雑な要件定義を正確に読み取り、逸脱せずに行動する能力。
2.  **自己検証メカニズムの強化:** 自分の行動結果を客観的にテストし、エラーから自律的に回復するループの確立。
3.  **環境認識能力:** ターミナルというステートフルな環境において、現在のディレクトリ構造やインストール済みパッケージの状態を正確に把握し続ける能力。

AIエージェントが真に信頼できるパートナーとなるために、Terminal-Benchのような厳格かつ現実的なベンチマークの存在は不可欠である。このベンチマークが飽和するその時こそ、AIエンジニアリングの新たな地平が開かれる瞬間となるだろう。

## 参考文献

*   Merrill, M. A., Shaw, A. G., et al. (2025). *Terminal-Bench: Benchmarking Agents on Hard, Realistic Tasks in Command Line Interfaces*. [PDF](file:///Users/jiwasawa/Dropbox/Papers/202504_paperpile/TERMINAL-BENCH-%20BENCHMARKING%20AGENTS%20ON%20HARD,%20REALISTIC%20TASKS%20IN%20COMMAND%20LINE%20INTERFACES.pdf)
*   Terminal-Bench Official Website: [tbench.ai](https://tbench.ai)
