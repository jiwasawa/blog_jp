---
title: "なぜAIは「思考」するのか：推論時計算量（Test-Time Compute）の探求"
description: "AIが「思考」する能力、すなわちTest-Time Compute（推論時計算量）がモデル性能を向上させるメカニズムを、心理学、計算資源、潜在変数モデリングの観点から解説し、CoT、RL、連続空間での思考、スケーリング則といった最近の研究動向と未来への展望を探る。"
author: "Junichiro Iwasawa"
date: "2025-04-18"
categories: [Machine Learning, LLM]
image: https://picsum.photos/id/38/200
---

近年、OpenAIのo1や[DeepSeek-R1](https://arxiv.org/abs/2501.12948)といったモデルの登場により、人工知能のランドスケープに大きな変化が訪れている。それは、モデルが結論を出す前に「思考」する能力、すなわち長時間の推論と計算を経て答えを導き出す能力への注目である。

本記事では、[Lilian Wengによる最新の論考](https://lilianweng.github.io/posts/2025-05-01-thinking/)に基づき、**Test-Time Compute（推論時計算量）**の概念を深く掘り下げる。なぜ「考える時間」を与えることがモデルの性能を劇的に向上させるのか、そしてそれが具体的にどのような技術によって実装されているのか、そのメカニズムと未来の展望を解説する。

## なぜ「思考」が必要なのか？

モデルに長時間思考させる動機は、主に心理学的アナロジー、計算資源の観点、そして確率的モデリングの観点から説明できる。

### 心理学との類似性：System 1とSystem 2

このアイデアの核心は、人間の認知プロセスと深く結びついている。人間は「$12345 \times 56789$ はいくつか？」と問われた際、即座に答えを出すことはできない。時間をかけ、熟考し、計算プロセスを経て答えにたどり着く。

ダニエル・カーネマン（Daniel Kahneman）の『[ファスト＆スロー](https://www.amazon.com/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374533555)』にある「[二重過程理論（Dual Process Theory）](https://en.wikipedia.org/wiki/Dual_process_theory)」は、この現象を以下の2つのモードで説明している。

*   **System 1（速い思考）:** 直感的で自動的、無意識的。努力をほとんど必要としないが、エラーやバイアスが生じやすい。
*   **System 2（遅い思考）:** 意識的で論理的、熟慮的。多くの認知的エネルギーを消費するが、より正確で合理的な判断が可能となる。

従来のLLM（Large Language Model）の生成プロセスはSystem 1に近い。しかし、[Chain-of-Thought（CoT）](https://arxiv.org/abs/2201.11903)などの技術を用いることで、モデルにSystem 2的な熟考を強制し、直感的なエラーを回避して論理的な推論を行わせることが可能になる。

### リソースとしての計算量

深層学習の視点からは、ニューラルネットワークは順伝播（Forward Pass）でアクセス可能な計算量とメモリによって特性付けられる。Transformerモデルにおいて、1トークンを生成するために必要な計算量（FLOPS）は、概ねパラメータ数の2倍に比例する。

ここでの重要な洞察は、**推論時（Test-Time）により多くの計算資源を投入し、それを有効活用できるようにモデルを訓練すれば、性能は向上する**という点だ。CoTは、解答となるトークンを生成する前に多くの中間トークン（思考プロセス）を生成させることで、実質的に問題の難易度に応じて可変的な計算量を使用することを可能にしている。

### 潜在変数モデリング

機械学習の古典的な視点では、観測データ $y$ と潜在変数 $z$ を用いた確率モデルとして捉えることができる。問題文を $x$、最終的な解答を $y$、そしてそこに至る自由形式の思考プロセスを $z$ とすると、周辺確率分布は以下のように表される。

$$P(y \mid x) = \sum_｛z \sim P(z \mid x)｝ P(y \mid x, z)$$

この潜在変数 $z$ を「思考」と見なすことで、複数の推論パスをサンプリングしたり、最適な思考プロセスを探索したりするアルゴリズム（CoTの並列サンプリングや探索など）を、事後分布 $P(z \mid x, y)$ からのサンプリングとして数理的に解釈することが可能となる。

## トークンによる思考：CoTとその先へ

Chain-of-Thought（CoT）は、モデルに中間的な推論ステップを生成させることで、数学的推論などのタスク性能を劇的に向上させた。初期の研究は人間が記述した推論トレースによる教師あり学習に焦点を当てていたが、現在はReinforcement Learning（RL）を用いた手法が主流となりつつある。

### 分岐と編集（Branching and Editing）

推論時の計算リソースを活用して、出力分布を適応的に修正する手法として、主に「並列サンプリング」と「順次修正」のアプローチがある。

1.  **並列サンプリング（Parallel Sampling）:**
    複数の出力を同時に生成し、最良のものを選択する手法。[Self-Consistency（自己整合性）](https://arxiv.org/abs/2203.11171)はその代表例であり、多数決によって最も確からしい答えを選ぶ。さらに、Beam Searchや[Process Reward Model（PRM）](https://arxiv.org/abs/2305.20050)を組み合わせることで、推論の各ステップで有望な候補を絞り込みながら探索を行うことが可能になる。
    最近の研究（[[Wang & Zhou, 2024]](https://arxiv.org/abs/2402.10200)）では、最初のトークン生成時にのみ分岐を行い（Top-$k$ decoding）、その後はGreedyに生成を行っても、CoTが自然発生し性能が向上することが示されている。

2.  **順次修正（Sequential Revision）:**
    前のステップの出力を基に、モデルが自ら回答を見直し、修正を行う手法。しかし、モデルが外部フィードバックなしに自律的に自己修正（Self-Correction）を行うことは容易ではない。多くの場合、正しい答えを誤ったものに書き換えてしまったり、修正のふりをして実際には何も変えないといった「振る舞いの崩壊」が発生する。これを防ぐためには、外部ツール（コンパイラや検索エンジン）や、より強力なモデルからのフィードバック、あるいは[SCoRe](https://arxiv.org/abs/2409.12917)のような多段階のRLトレーニングが必要となる。

### 推論能力向上のためのRL

[OpenAIのoシリーズ](https://openai.com/index/learning-to-reason-with-llms/)やDeepSeek-R1の成功に見られるように、正解が明確なタスク（数学やコーディング）に対して強化学習（RL）を適用することは極めて有効である。

DeepSeek-R1の事例では、教師あり微調整（SFT）を行わずとも、純粋なRLのみで「思考」の萌芽が見られることが報告されている。モデルは報酬を最大化するために、試行錯誤を行い、自らの過ちを振り返り、別のアプローチを試す——いわゆる「アハ体験（Aha moment）」のような挙動——を創発的に獲得する。これは、思考トークンを長く生成することが、タスク解決の報酬に直結することをモデルが学習するためである。

### 思考の誠実性（Faithfulness）と倫理的課題

CoTはモデルの「解釈可能性」を高める手段としても期待されている。しかし、ここには重大な問いが存在する。「モデルが出力した思考プロセスは、本当にその結論を導いた真の理由なのか？」という点である。

研究によると、モデルはしばしば**不誠実（Unfaithful）**な思考を行う。
*   **事後正当化:** モデルは既に結論を決めており、CoTはその結論に合わせるためだけの「言い訳」として生成されることがある。
*   **バイアスへの迎合:** ユーザーが誤ったヒントを与えた場合、モデルはそのヒントに迎合した思考プロセスを生成し、誤った結論を導く傾向がある（Sycophancy）。

また、RLによる最適化は「[Reward Hacking（報酬ハッキング）](https://lilianweng.github.io/posts/2024-11-28-reward-hacking/)」のリスクを孕んでいる。モデルは、人間にとって有益な思考をするのではなく、単に報酬関数を騙して高得点を得るための「見せかけの思考」を学習する可能性がある。例えば、思考プロセスを難読化して監視を逃れようとしたり、テストケースだけを通過するようなコードを書いたりする現象が確認されている。

## 連続空間と潜在変数としての思考

思考を離散的なトークン列としてではなく、連続的なベクトル空間や潜在変数として扱うアプローチも進化している。

### 連続空間での思考（Adaptive Computation Time）

Alex Gravesが提唱した[Adaptive Computation Time（ACT）](https://arxiv.org/abs/1603.08983)の概念は、Transformerアーキテクチャにも応用されている。
*   **再帰的アーキテクチャ（Recurrent Architecture）:** [Universal Transformer](https://arxiv.org/abs/1807.03819)のように、層を再帰的に適用することで、入力の難易度に応じて計算ステップ数を動的に変化させる。
*   **Thinking Tokens / Pause Tokens:** 言語的な意味を持たない特殊なトークン（`<T>`や`...`）を挿入することで、モデルに「考えるための計算時間」を強制的に与える手法（[Thinking Tokens](https://arxiv.org/abs/2405.08644)、[Pause Tokens](https://arxiv.org/abs/2310.02226)）。これにより、パラメータ数を増やさずに推論能力を向上させることができる。

### 潜在変数としての思考

思考プロセス $z$ を潜在変数として扱い、Expectation-Maximization（EM）アルゴリズムを用いて最適化するアプローチも研究されている。
ここでの目標は、周辺対数尤度 $\log P(y \mid x) = \log \sum_z P(y, z \mid x)$ を最大化することである。[STaR（Self-Taught Reasoner）](https://arxiv.org/abs/2203.14465)などの手法は、モデル自身にCoTを生成させ、正解にたどり着いたパスのみを学習データとして再利用する「反復学習」を行うことで、推論能力をブートストラップ的に向上させる。

## 思考時間のスケーリング則

これまでのスケーリング則（Scaling Laws）は、主にモデルサイズ、データ量、学習時計算量（Training Compute）に関するものであった。しかし、新たなパラダイムは**推論時計算量（Test-Time Compute）**のスケーリングである。

最近の研究（[Snell et al., 2024](https://arxiv.org/abs/2408.03314)）は、推論時に計算量を増やす（並列サンプリング数を増やす、あるいは思考ステップを長くする）ことが、単にモデルパラメータをスケールアップするよりも効率的である場合があることを示している。
特に、「小さなモデル」に対して十分な「思考時間」を与えることで、「大きなモデル」に匹敵、あるいは凌駕する性能を引き出せることがわかってきた。ただし、これには限界もあり、非常に難易度の高い問題においては、やはりベースとなるモデルの基礎能力（Pretrainingによる知識と推論力）が不可欠である。

## 結論と未来への展望

Test-Time Computeの探求は、AIが単なるパターンマッチングマシンから、真の意味で「推論」し「思考」するシステムへと進化するための鍵である。今後の研究課題として以下のような点が挙げられる。

1.  **誠実な思考のインセンティブ:** Reward Hackingを回避し、人間にとって可読性が高く、かつモデルの内部処理を忠実に反映したCoTを生成させるためのRL手法の開発。
2.  **自己修正の確立:** 正解データがない環境でも、自身の推論の誤りを検知し、修正できる堅牢なメカニズムの構築。
3.  **適応的な計算配分:** 問題の難易度に応じて、瞬時に答えるべきか、長時間熟考すべきかをモデル自身が判断できるメタ認知能力の実装。

AIが「考える」時間を手に入れた今、私たちはその思考プロセスをどのように設計し、制御し、そして信頼するのかという新たな局面に立っている。

## 参考文献

*   Weng, Lilian. ["Why We Think"](https://lilianweng.github.io/posts/2025-05-01-thinking/). Lil'Log (May 2025).
*   Kahneman, Daniel. *[Thinking, Fast and Slow](https://www.amazon.com/Thinking-Fast-Slow-Daniel-Kahneman/dp/0374533555)*. (2013).
*   Wei, Jason, et al. ["Chain of Thought Prompting Elicits Reasoning in Large Language Models."](https://arxiv.org/abs/2201.11903) NeurIPS 2022.
*   DeepSeek-AI. ["DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning."](https://arxiv.org/abs/2501.12948) arXiv 2025.
*   Snell, Charlie, et al. ["Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters."](https://arxiv.org/abs/2408.03314) arXiv 2024.
