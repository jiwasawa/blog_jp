---
title: "リーダーボードという名の幻影：LMArenaは信じられるのか？"
description: "LLM評価の定番LMArenaは本当に信頼できるのか？ 話題の批判論文「The Leaderboard Illusion」を軸に、その公平性やランキングの「幻影」の正体を考察します。"
author: "Junichiro Iwasawa"
date: "2025-05-01"
categories: [LLM, AI]
image: https://picsum.photos/id/87/200
---

LLM（大規模言語モデル）開発競争が激化する昨今、どのモデルが「最も優れているか」を示す指標として、Chatbot Arena（LMArena）のリーダーボードがデファクトスタンダードとしての地位を確立しつつある。ユーザーが二つの匿名モデルの回答を比較評価するというシンプルな仕組みと、日々更新されるランキングが、開発者・研究者・メディアから絶大な注目を集めているわけだ。まるでLLM界の総選挙。その結果に一喜一憂する光景も、もはや日常となりつつある。

しかし、その「民意」を反映するとされるランキングの信頼性に、真っ向から疑問を投げかける論文が登場した。[「The Leaderboard Illusion」](https://arxiv.org/abs/2504.20879)と題されたこの研究は、Cohereの研究者らを中心にまとめられ、LMArenaの運用に潜む体系的な問題点を鋭く指摘している。今回はこの論文と、それに対するLMArena運営（lmarena.ai）や業界識者（Andrej Karpathy氏）の反応を元に、LMArenaランキングの「幻影」の正体に迫ってみたい。

## 「リーダーボードの幻影」論文が暴いたLMArenaの歪み

この論文ではなかなか鋭い指摘がされている。要約すると、LMArenaはその公平性・透明性を謳いつつも、特定のプレイヤー（特に大手プロプライエタリモデル提供者）に有利な構造が出来上がっており、ランキングがモデルの真の実力を反映していない可能性がある、という主張だ。主な論点は以下の4つに集約される。

1.  **不公平なプライベートテストと結果の選択的開示:**
    LMArenaには、特定の（主に大手の）プロバイダーが、公式リリース前に多数のモデルバリアントを「非公開」でテストできる、公にはされていないポリシーが存在するという。例えば、Meta社はLlama 4リリース前に27もの非公開モデルをテストしていたことが観測されている。問題は、これらのテスト結果の中から最もスコアが良かったものだけを選んで公開（あるいは公開モデルのバージョンとして採用）できる点だ。これは統計的な偏りを生み、本来の実力以上にスコアを「かさ上げ」する効果がある。論文では、この「best-of-N戦略」がBradley-Terry (BT)モデル（LMArenaのスコアリングに使われる統計モデル）の前提を崩し、ランキングを歪めているとシミュレーションと実証実験（Cohere自身も実験のために非公開モデルを投入）で示している。

2.  **データアクセスにおける著しい格差:**
    LMArenaはクラウドソースによる評価プラットフォームだが、ユーザーが入力したプロンプトや評価結果といった貴重なデータへのアクセス権が、プロバイダー間で著しく偏っている。論文の推計によると、GoogleとOpenAIだけで全バトルデータのそれぞれ約19.2%、20.4%を受け取っている一方、83ものオープンウェイトモデル群全体では29.7%に過ぎない。この格差は、①前述の非公開テストの多さ、②モデルがバトルに登場する頻度（サンプリングレート）の偏り（プロプライエタリモデルの方が高い傾向）、③モデルの「非推奨化（deprecation）」ポリシー（オープン系モデルの方が非アクティブ化されやすい傾向）によって生まれているという。コミュニティの「無料奉仕」が、一部の巨大テック企業に偏って還元されている構図だ。

3.  **アリーナへの過剰適合（Overfitting）リスク:**
    データアクセス格差は、単なる不公平感にとどまらない。論文では、LMArenaのバトルデータ（アリーナ特有のプロンプト傾向を持つ）を使ってモデルをファインチューニングすると、アリーナ上でのパフォーマンス（勝率）が劇的に向上することを示している（実験では最大112%の相対的向上）。しかし、その効果はアリーナ外の一般的なベンチマーク（MMLUなど）には波及せず、むしろスコアが低下する場合すらあった。これは、モデルが真に賢くなるのではなく、「LMArenaで勝つためのテクニック」に過剰適合している可能性を示唆している。「アリーナ番長」を作り出す土壌になっているのではないか、というわけだ。

4.  **モデル削除方針とランキング信頼性の低下:**
    LMArenaでは古いモデルや性能の低いモデルが非推奨化され、バトルから除外されていく。論文によると、公式に非推奨とされているモデルは47だが、実際には205ものモデルが（多くは通知なく）実質的に非アクティブ化されているという。特にオープン系のモデルが多く除外される傾向にある。問題は、モデルが頻繁に入れ替わり、かつ評価されるプロンプトの傾向も時間と共に変化する中で、特定のモデルが早期に評価対象から外れると、BTモデルが前提とする比較の網羅性や推移性（A>B, B>C ならば A>C）が崩れ、ランキング全体の信頼性が損なわれる可能性があることだ。過去の栄光にしがみつく古豪と、最新の環境で評価される新鋭との比較が、実はフェアではないかもしれない。

## LMArena運営とKarpathy氏の反応：それぞれの言い分

この痛烈な批判に対し、LMArena運営チームも[Xで反論](https://x.com/lmarena_ai/status/1917492084359192890)している。曰く、

* 事前テストは、プロバイダーが「コミュニティ（ユーザー）が最も好むバリアント」を見つける手助けになるだけで、リーダーボードを歪めるものではない。
* リーダーボードは、数百万の新鮮でリアルな人間の好みを反映している。主観的な好みこそが重要。
* データアクセスによってモデルが人々の好みに最適化されるなら、それはポジティブなことだ。
* 事前テストは全てのプロバイダーに開かれており、誰かを不公平に扱っているわけではない。利用するかどうかは各社の判断。
* 論文のシミュレーションは欠陥があり（ステフィン・カリーの3ポイントシュート成功率を例にした皮肉）、数値にも事実誤認がある。
* 我々はオープンソース開発を支援している（プラットフォームやデータ公開など）。
* 論文の提案の一部（アクティブサンプリング導入など）は検討に値する。

要するに、「我々のやり方は透明で公平。ランキングはリアルなユーザー評価の表れであり、問題ない。論文は勘違いしている部分が多い」というスタンスである。

一方で、著名なAI研究者であるAndrej Karpathy氏は、この論文を受けて[興味深いコメント](https://x.com/karpathy/status/1917546757929722115)を寄せている。

* 以前からLMArenaのランキングには個人的な違和感があった。Geminiのあるモデルが一時トップになったが、実際に使ってみると期待外れだった。逆にClaude 3.5は個人的には非常に良かったが、当初アリーナでのランクは低かった。
* データと個人の体験談が食い違うときは、体験談の方が正しいことが多い（ジェフ・ベゾスの言葉を引用）。
* 各チームがLMArenaのスコアを過度に意識し、汎用的なモデル改善ではなく「LMArenaでスコアが高くなるモデル」（やたらリストや箇条書き、絵文字を使うような？）を作ることに注力している可能性がある。
* LMArenaも改善は続けるだろうが、代替としてOpenRouterのランキング（実際のAPI利用量やコストに基づき、ユーザーが実利でモデルを選んでいる）が有望かもしれない。

Karpathy氏のコメントは、論文が指摘する「アリーナへの過剰適合」や「ランキングと実用性の乖離」といった懸念を、ユーザー視点の「体感」として裏付けているようで興味深い。

## 考察：ランキングの向こう側に見えるもの

さて、ここまで論文の指摘と関係者の反応を見てきた。LMArena運営側の反論は、コミュニティの好みを尊重するという理念は理解できるものの、論文が核心として指摘する「選択的報告によるスコアの歪み」「データアクセスの極端な偏り」「過剰適合のリスク」といったメカニズムに対して、十分に答えているとは言い難いのではないか。特に「テストは公平に開かれている」という主張も、実際には情報格差やリソース格差によって、大手プロバイダーが圧倒的に有利な状況が生まれている実態を覆い隠してはいないだろうか。

Karpathy氏の「体感とのズレ」や「アリーナ特化最適化」への疑念は、まさに論文がデータで示そうとした問題点を補強しているように思える。「コミュニティの好み」が、特定のタスクやスタイルに偏ったものであり、それを最適化することが必ずしもLLMの汎用的な能力向上に繋がらないのだとしたら、LMArenaは我々をミスリードしている可能性すらある。それはもはや「好み」の問題ではなく、「評価指標としての妥当性」の問題だ。

正直、大手テック企業が潤沢なリソースを背景に、LMArenaという「ゲーム」のルールを最大限利用してランキング上位を狙う、いわゆる「ゲーミフィケーション」が起きている可能性は否定できないだろう。それが健全な競争と言えるのかどうか。

## 結論と提言：より良い評価のために必要なこと

LMArenaがLLM評価に果たしてきた役割、特にユーザー参加型の評価というコンセプトの価値は大きい。しかし、「リーダーボードの幻影」論文が明らかにしたように、その運用には重大な懸念が存在するのも事実だ。

論文が提言する改善策——スコア撤回の禁止、非公開テスト数の制限と透明化、公平で監査可能なモデル削除基準の策定、不確実性を減らすための公平なサンプリング（彼らが過去に提案したアクティブサンプリングの導入）、そして各種運用情報の完全な透明化——は、いずれもリーダーボードの信頼性を回復するために不可欠なステップだろう。

我々研究者、開発者、そしてユーザーは、LMArenaのような単一のリーダーボードの順位を鵜呑みにするのではなく、多角的な視点を持つことが重要だ。Karpathy氏が示唆するように、実際のユースケースやコストパフォーマンスに基づいた評価もまた、重要な判断材料となる。

AI分野全体の健全な発展のためには、評価手法そのものが常に批判的に吟味され、改善され続ける必要がある。LMArena運営チームには、今回の指摘を真摯に受け止め、より公平で透明性の高いプラットフォームへと進化していくことを期待したい。さもなければ、「リーダーボードの幻影」は、我々の進むべき道を見誤らせる蜃気楼になりかねない。それは、AI開発に関わる全ての人にとって、あまりにも大きな損失だろう。
